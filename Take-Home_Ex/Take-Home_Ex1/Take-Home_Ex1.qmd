---
title: "Geospatial Analytics for Public Good"
author: "Su Sandi Cho Win"
date: "25 November 2023"
date-modified: "25 November 2023"
format: html
execute:
  echo: true
  eval: true
  warning: false
editor: visual
---

## 1. Overview

The digital transformation of urban infrastructure, including buses, taxis, mass transit, utilities, and roads, has generated vast datasets that serve as a foundation for tracking movement patterns over space and time. Pervasive technologies like GPS and RFID, widely adopted in vehicles, contribute to this trend. For example, smart cards and GPS devices collect route and ridership data on public buses. These extensive datasets likely contain valuable patterns that offer insights into observed phenomena. By deepening our understanding of human mobility in urban areas can improve urban management and provide vital information for both public and private urban transport providers, enhancing their decision-making and competitiveness.

In practical terms, the utilization of these extensive location-aware datasets has predominantly been confined to rudimentary tracking and mapping using Geographic Information System (GIS) applications. This limitation stems primarily from the conventional GIS's inherent lack of robust capabilities for effectively analyzing and modeling spatial and spatio-temporal data.

### 1.1. Objective

Exploratory Spatial Data Analysis (ESDA) represents a powerful approach with the capacity to tackle intricate societal challenges. Within the scope of this study, the objective is to employ suitable Local Indicators of Spatial Association (GLISA) and Emerging Hot Spot Analysis (EHSA) methodologies. This will enable us to unveil the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.

## 2. Getting Started

The following code chunk installs and loads **sf**, **sfdep**, **tmap**, **tidyverse, knitr, dplyr, hexbin** packages into R environment. [*pacman()*](https://cran.r-project.org/web/packages/pacman/readme/README.html) is a R package management tool.

```{r}
pacman::p_load(sf, sfdep, tmap, tidyverse, plotly, knitr, dplyr, hexbin)
```

## 3. Data Preparation

### 3.1. Data

The datasets used for this study are:

-   Master Plan 2019 Planning Sub-zone Geographic Information Systems (GIS) data set of URA from [data.gov.sg](https://beta.data.gov.sg/),

-   Passenger Volume by Origin Destination Bus Stops from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html),

-   Bus Stop Location dataset from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html)

### 3.2. Importing the Data into R Environment

#### **3.2.1. Importing Geospatial Data into R**

```{r}
busstop <- st_read(dsn = "data/geospatial",
                   layer = "BusStop") %>%
  st_transform(crs = 3414)
glimpse(busstop)
```

import *MPSZ-2019* downloaded from eLearn into RStudio and save it as a sf data frame called `mpsz`.

```{r}
mpsz <- st_read(dsn = "data/geospatial",
                   layer = "MPSZ-2019") %>%
  st_transform(crs = 3414)
```

#### **3.3.2. Importing Attribute Data into R**

Firstly, we will import the *Passenger Volume by Origin Destination Bus Stops* data set downloaded from LTA DataMall by using `read_csv()` of **readr** package.

```{r}
odbus <- read_csv("data/aspatial/origin_destination_bus_202308.csv")
glimpse(odbus)
```

```{r}
# Using tidyverse functions to convert these data values into factor data type.
odbus$ORIGIN_PT_CODE <- as.factor(odbus$ORIGIN_PT_CODE)
odbus$DESTINATION_PT_CODE <- as.factor(odbus$DESTINATION_PT_CODE) 
glimpse(odbus)
```

#### 

#### 3.3.3. **Extracting the study data**

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
origin6_9 <- odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 6 &
           TIME_PER_HOUR <= 9) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

kable(head(origin6_9))
```

We will save the output in rds format for future use.

```{r}
write_rds(origin6_9, "data/rds/origin6_9.rds")
```

The follwing code chunk will be used to import the saved origin6_9.rds into R environment.

```{r}
origin6_9 <- read_rds("data/rds/origin6_9.rds")
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
origin17_20 <- odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 17 &
           TIME_PER_HOUR <= 20) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

kable(head(origin17_20))
```

We will save the output in rds format for future use.

```{r}
write_rds(origin17_20, "data/rds/origin17_20.rds")
```

The following code chunk will be used to import the saved origin17_20. rds into R environment.

```{r}
origin17_20 <- read_rds("data/rds/origin17_20.rds")
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
origin11_14 <- odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 11 &
           TIME_PER_HOUR <= 14) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

kable(head(origin11_14))
```

We will save the output in rds format for future use.

```{r}
write_rds(origin11_14, "data/rds/origin11_14.rds")
```

The following code chunk will be used to import the saved origin11_14.rds into R environment.

```{r}
origin11_14 <- read_rds("data/rds/origin11_14.rds")
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
origin16_19 <- odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 16 &
           TIME_PER_HOUR <= 19) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

kable(head(origin16_19))
```

We will save the output in rds format for future use.

```{r}
write_rds(origin16_19, "data/rds/origin16_19.rds")
```

The following code chunk will be used to import the saved origin16_19.rds into R environment.

```{r}
origin16_19 <- read_rds("data/rds/origin16_19.rds")
```
:::

### **3.4. Data Wrangling - Geospatial Data**

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
origin6_9_busstop <- left_join(busstop, origin6_9,
            by = c("BUS_STOP_N" = "ORIGIN_PT_CODE"))
origin6_9_busstop
```

```{r}
duplicate <- origin6_9_busstop %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
origin17_20_busstop <- left_join(busstop, origin17_20,
            by = c("BUS_STOP_N" = "ORIGIN_PT_CODE"))
origin17_20_busstop
```

```{r}
duplicate <- origin17_20_busstop %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
origin11_14_busstop <- left_join(busstop, origin11_14,
            by = c("BUS_STOP_N" = "ORIGIN_PT_CODE"))
origin11_14_busstop
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
origin16_19_busstop <- left_join(busstop, origin16_19,
            by = c("BUS_STOP_N" = "ORIGIN_PT_CODE"))
origin16_19_busstop
```
:::

## 4. Exploratory Data Analysis

Preparing a reusable function to process input data

```{r}
honeycomb_grid <- function(input_data) {
  area_honeycomb_grid_current <-
    st_make_grid(input_data, c(500), what = "polygon", square = FALSE)
  
  # To sf and add grid ID
  honeycomb_grid_sf_current <- st_sf(area_honeycomb_grid_current) %>%
    # add grid ID
    mutate(grid_id = 1:length(lengths(area_honeycomb_grid_current)))
  
  # Perform spatial intersection to count points within each hexagon
  point_counts_current <-
    st_intersection(honeycomb_grid_sf_current, input_data) %>%
    group_by(grid_id) %>%
    summarize(TOT_TRIPS = sum(TRIPS, na.rm = TRUE))  # Replace 'TRIPS' with your column name
  
  # Merge the point counts back into the honeycomb grid
  honeycomb_grid_sf_current <- honeycomb_grid_sf_current %>%
    st_join(point_counts_current, by = "grid_id")
  
  # remove grid without value of 0 (i.e. no points in side that grid)
  honeycomb_count_current <-
    filter(honeycomb_grid_sf_current, TOT_TRIPS > 0)
  
  return(honeycomb_count_current)
}
```

Preparing a reusable function to plot the honeycomb map

```{r}
draw_honeycomb_map <- function(input_data) {
  map_output <- tm_shape(input_data) +
    tm_fill(
      breaks = c(0, 2000, 5000, 10000, 20000, 500000),
      labels = c("0 to 1999", "2000 to 4999", "5000 to 9999", "10000 to 19999", "20000 to 499999"),
      col = "TOT_TRIPS",
      palette = "Reds",
      style = "fixed",
      title = "Number of Trips",
      id = "grid_id",
      showNA = FALSE,
      alpha = 0.6,
      popup.vars = c(
        "Number of trips: " = "TOT_TRIPS"
      ),
      popup.format = list(
        TOT_TRIPS = list(format = "f", digits = 0)
      )
    ) +
    tm_borders(col = "grey40", lwd = 0.7) +
    tmap_mode("view")

  return (map_output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
honeycomb_grid_6_9 <- honeycomb_grid(origin6_9_busstop)
draw_honeycomb_map(honeycomb_grid_6_9)
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
honeycomb_grid_17_20 <- honeycomb_grid(origin17_20_busstop)
draw_honeycomb_map(honeycomb_grid_17_20)
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
honeycomb_grid_11_14 <- honeycomb_grid(origin11_14_busstop)
draw_honeycomb_map(honeycomb_grid_11_14)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
honeycomb_grid_16_19 <- honeycomb_grid(origin16_19_busstop)
draw_honeycomb_map(honeycomb_grid_16_19)
```
:::

## 5. Exploratory Spatial Data Analysis

Exploratory Spatial Data Analysis (ESDA) is an expansion of the Exploratory Data Analysis concept, encompassing descriptive methods aimed at unveiling the spatial distribution of data and detecting anomalies.

In this segment, we will delve into both global and local spatial autocorrelation. The global perspective highlights overarching trends, whereas the local analysis empowers us to pinpoint regions of heightened and reduced values within the dataset.

### 5.1. **Global Spatial Autocorrelation**

In accordance with Tobler's First Law of Geography, it is acknowledged that "all things are interconnected, but objects in close proximity exhibit stronger connections than those farther apart.".

In this subsection, we will delve into the calculation of global spatial autocorrelation statistics and the assessment of spatial randomness on a global scale. The objective of these analyses is to gain insights into the distribution of passenger volume by Origin Destination Bus Stops across Singapore, assessing whether they are uniformly dispersed or not.

#### 5.1.1. **Spatial Weights Matrix**

To perform global spatial autocorrelation analysis, our initial step involves the creation of spatial weights for Singapore to establish the spatial neighborhood, a fundamental element for subsequent spatial analysis. There are two prevalent approaches for generating spatial weights: contiguity-based and distance-based methods.

In contiguity-based methods, neighboring areas are determined based on shared boundaries, with variations in how these boundaries are defined in different methods. Rook contiguity, for instance, considers neighbors that share a common edge. Conversely, Queen contiguity deems neighbors as those who share either a common vertex or edge. Consequently, Queen contiguity is more inclusive compared to Rook contiguity.

The distinctions between these two approaches are elucidated in the following illustration.

![](images/Comparing-rook-and-queen-contiguity-Each-square-is-representative-of-geographic-shapes.ppm){width="650"}

In distance-based contiguity, there are two distinct weighting schemes: fixed weighting and adaptive weighting. The former approach deems two regions as neighbors if they fall within a predetermined distance of each other. In the latter scheme, every region is assigned an equal number of neighbors, with the specific number of neighbors predetermined. For instance, if we set k = 8 neighbors, it will categorize the nearest eight regions as neighbors for each.

The choice of which spatial weights method to employ hinges on the geographical characteristics of the area under consideration. In cases where the geographical area comprises numerous isolated areas, opting for a contiguity-based matrix might lead to many regions lacking neighbors. Similarly, if the features (polygons) exhibit a wide range of sizes, encompassing both very large and relatively smaller ones, the use of a contiguity-based matrix may result in larger features having a disproportionate number of neighbors. This could potentially introduce a smoothing effect due to the higher number of neighbors for larger features, potentially skewing the analytical outcomes.

#### 5.1.2. **Contiguity-based Spatial Weights**

This section will focus on the creation of contiguity spatial weights using the **sfdep** package. Deriving contiguity spatial weights involves two key steps:

1.  Identifying the contiguity neighbor list using the [`st_contiguity()`](https://sfdep.josiahparry.com/reference/st_contiguity.html) function from the **sfdep** package.

2.  Generating the contiguity spatial weights using the [`st_weights()`](https://sfdep.josiahparry.com/reference/st_weights.html) function, also part of the **sfdep** package.

Throughout this section, we will delve into the process of obtaining the contiguity neighbor list and the contiguity spatial weights separately. Subsequently, we will explore how to seamlessly combine both steps into a single procedure.

#### 5.1.2.1. **Contiguity-based (Queen) Spatial Weight Contiguity**

The following code chunk utilizes [`st_contiguity()`](https://sfdep.josiahparry.com/reference/st_contiguity.html) to derive a contiguity neighbour list by using Queen's method.

```{r}
nb_queen <- function(input_data){
  output_nb_queen <- input_data %>% 
    mutate(nb = st_contiguity(area_honeycomb_grid_current),
         .before = 1)
  
  return(output_nb_queen)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
nb_queen_6_9 <- nb_queen(honeycomb_grid_6_9)
summary(nb_queen_6_9$nb)
```

The summary report above shows that there are 1493 area units in Singapore. The most connected area units have 6 neighbors. There are 40 area units with only one neighbor.

```{r}
kable(head(nb_queen_6_9,
           n=10))
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
nb_queen_17_20 <- nb_queen(honeycomb_grid_17_20)
summary(nb_queen_17_20$nb)
```

The summary report above shows that there are 1495 area units in Singapore. The most connected area units have 6 neighbors. There are 37 area units with only one neighbor.

```{r}
kable(head(nb_queen_17_20,
           n=10))
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
nb_queen_11_14 <- nb_queen(honeycomb_grid_11_14)
summary(nb_queen_11_14$nb)
```

The summary report above shows that there are 1499 area units in Singapore. The most connected area units have 6 neighbors. There are 41 area units with only one neighbor.

```{r}
kable(head(nb_queen_11_14,
           n=10))
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
nb_queen_16_19 <- nb_queen(honeycomb_grid_16_19)
summary(nb_queen_16_19$nb)
```

The summary report above shows that there are 1489 area units in Singapore. The most connected area units have 6 neighbors. There are 42 area units with only one neighbor.

```{r}
kable(head(nb_queen_16_19,
           n=10))
```
:::

#### 5.1.2.2. **Contiguity-based (Rook) Spatial Weight Contiguity**

```{r}
nb_rook <- function(input_data){
  output_nb_rook <- input_data %>% 
    mutate(nb = st_contiguity(area_honeycomb_grid_current,
                              queen = FALSE),
         .before = 1)
  
  return(output_nb_rook)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
nb_rook_6_9 <- nb_rook(honeycomb_grid_6_9)
summary(nb_rook_6_9$nb)
```

The summary report above shows that there are 1493 area units in Singapore. The most connected area units have 6 neighbors. There are 40 area units with only one neighbor.

```{r}
kable(head(nb_rook_6_9,
           n=10))
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
nb_rook_17_20 <- nb_rook(honeycomb_grid_17_20)
summary(nb_rook_17_20$nb)
```

The summary report above shows that there are 1495 area units in Singapore. The most connected area units have 6 neighbors. There are 37 area units with only one neighbor.

```{r}
kable(head(nb_rook_17_20,
           n=10))
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
nb_rook_11_14 <- nb_rook(honeycomb_grid_11_14)
summary(nb_rook_11_14$nb)
```

The summary report above shows that there are 1499 area units in Singapore. The most connected area units have 6 neighbors. There are 41 area units with only one neighbor.

```{r}
kable(head(nb_rook_11_14,
           n=10))
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
nb_rook_16_19 <- nb_rook(honeycomb_grid_16_19)
summary(nb_rook_16_19$nb)
```

The summary report above shows that there are 1489 area units in Singapore. The most connected area units have 6 neighbors. There are 42 area units with only one neighbor.

```{r}
kable(head(nb_rook_16_19,
           n=10))
```
:::

#### 5.1.3. **Distance-based Contiguity Weight Matrix**

There are three commonly employed types of distance-based spatial weights:

1.  Fixed distance weights

2.  Adaptive distance weights

3.  Inverse distance weights (IDW)

#### 5.1.3.1. **Fixed Distance Weight Matrix**

Now, we will go ahead to derive summary statistics of the nearest neighbor distances vector (i.e. dists) by using the following code chunk.

```{r}
wmfd_dists <- function(input_data){
  geo <- sf::st_geometry(input_data)
  nb <- st_knn(geo, longlat = TRUE)
  dists <- unlist(st_nb_dists(geo, nb))
  # summary(dists)
  return(dists)
}

tab_content <- function(input_data){
  result <- summary(wmfd_dists(input_data))
  return(result)
}
```

The majority of hexagons seem to be uniformly spaced at 500 units apart since the minimum, first quartile, median, and third quartile are all the same (500 units). This indicates a high level of uniformity in the spacing of most hexagons. The maximum value (4582.6 units) is an outlier compared to the other values. It suggests that there is at least one pair of hexagons that are significantly further apart than the rest.

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
tab_content(honeycomb_grid_6_9)
```

The majority of hexagons seem to be uniformly spaced at 500 units apart since the minimum, first quartile, median, and third quartile are all the same (500 units). This indicates a high level of uniformity in the spacing of most hexagons. The maximum value (4582.6 units) is an outlier compared to the other values. It suggests that there is at least one pair of hexagons that are significantly further apart than the rest.

## Weekday Afternoon Peak 5pm to 9pm

```{r}
tab_content(honeycomb_grid_17_20)
```

The majority of hexagons seem to be uniformly spaced at 500 units apart since the minimum, first quartile, median, and third quartile are all the same (500 units). This indicates a high level of uniformity in the spacing of most hexagons. The maximum value (4582.6 units) is an outlier compared to the other values. It suggests that there is at least one pair of hexagons that are significantly further apart than the rest.

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
tab_content(honeycomb_grid_11_14)
```

The majority of hexagons seem to be uniformly spaced at 500 units apart since the minimum, first quartile, median, and third quartile are all the same (500 units). This indicates a high level of uniformity in the spacing of most hexagons. The maximum value (4583 units) is an outlier compared to the other values. It suggests that there is at least one pair of hexagons that are significantly further apart than the rest.

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
tab_content(honeycomb_grid_16_19)
```

The majority of hexagons seem to be uniformly spaced at 500 units apart since the minimum, first quartile, median, and third quartile are all the same (500 units). This indicates a high level of uniformity in the spacing of most hexagons. The maximum value (4583 units) is an outlier compared to the other values. It suggests that there is at least one pair of hexagons that are significantly further apart than the rest.
:::

The summary statistics report above shows that the maximum nearest neighbor distance is 4583 units. By using a threshold value of 4583 units will ensure that each area will have at least one neighbor. Now we will go ahead to compute the fixed distance weights by using the code chunk below.

```{r}
wm_fd <- function(input_data) {
  output <- input_data %>%
    mutate(nb = st_dist_band(area_honeycomb_grid_current,
                        upper = 4583),
      wt = st_weights(nb),
      .before = 1)
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
wm_fd(honeycomb_grid_6_9)
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
wm_fd(honeycomb_grid_17_20)
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
wm_fd(honeycomb_grid_11_14)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
wm_fd(honeycomb_grid_16_19)
```
:::

#### 5.1.3.2. **Adaptive Distance-based Weight Matrix**

To address the problem of an uneven distribution of neighbors in a fixed distance weight matrix, we can directly control the number of neighbors by employing the k-nearest neighbors approach, as demonstrated in the following code snippet.

For hexagonal grid, we will set k = 6, meaning that each region will be assigned exactly 6 neighbors.

```{r}
wm_ad <- function(input_data) {
  output <- input_data %>%
    mutate(nb = st_knn(area_honeycomb_grid_current,
                  k = 6),
      wt = st_weights(nb),
      .before = 1)
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
wm_ad(honeycomb_grid_6_9)
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
wm_ad(honeycomb_grid_17_20)

```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
wm_ad(honeycomb_grid_11_14)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
wm_ad(honeycomb_grid_16_19)
```
:::

#### 5.1.3.3. Inverse distance weights (IDW)

The following code chunk utilizes:

-   [`st_contiguity()`](https://sfdep.josiahparry.com/reference/st_contiguity.html) to identify the neighbors by using contiguity criteria. The output is a list of neighbors (i.e. nb).

-   [`st_inverse_distance()`](https://sfdep.josiahparry.com/reference/st_inverse_distance.html) to calculate inverse distance weights of neighbors on the nb list.

```{r}
wm_idw <- function(input_data) {
  output <- input_data %>%
    mutate(
      nb = st_contiguity(area_honeycomb_grid_current),
      wts = st_inverse_distance(nb,
        area_honeycomb_grid_current,
        scale = 1,
        alpha = 1),
      .before = 1)
  
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
wm_idw(honeycomb_grid_6_9)
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
wm_idw(honeycomb_grid_17_20)
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
wm_idw(honeycomb_grid_11_14)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
wm_idw(honeycomb_grid_16_19)
```
:::

#### 5.1.4. **Which spatial weight matrix to use?**

The choice of a spatial weight matrix to use depends on the geographical area under investigation and the research objectives. In our specific context, when considering options between contiguity-based and distance-based spatial weight matrices, we are inclined towards the use of distance-based matrices. Within the category of distance-based matrices, we will specifically opt for the adaptive distance-based spatial weight matrix for our forthcoming analyses.

Here are the summarized reasons for our choice:

1.  Since the uniform grid has been disrupted by removing hexagons, an adaptive approach (like k-nearest neighbors) will be more appropriate. This method ensures that each hexagon has a set number of neighbors, compensating for any irregularities in the grid caused by the removal of zero-trip areas.

2.  The adaptive method can effectively deal with the non-contiguity issue, ensuring that each hexagon still considers an appropriate neighborhood set, even if some adjacent hexagons are missing.

3.  With missing hexagons, contiguity-based methods (like Rook or Queen) might not accurately reflect the spatial relationships because they rely on the assumption of a continuous, uninterrupted grid.

Taking these factors into account, our selection will be the adaptive distance-based spatial weight matrix.

#### 5.1.5. **Row-Standardized Weights Matrix**

With the weight matrix chosen, the next step is to assign weights to each neighboring polygon. In this process, every neighboring polygon will be given equal weight (style="W") by allocating the fraction 1/(number of neighbors) to each adjacent area. This corresponds to what is referred to as a row-standardized matrix, where each row in the matrix totals to 1.

```{r}
rswm_ad <- function(input_data) {
  output <- input_data %>%
    mutate(nb = st_knn(area_honeycomb_grid_current,
                  k = 4),
      wt = st_weights(nb,
                      style = "B"),
      .before = 1)
  return(output)
}

tab_content <- function(input_data){
  return(rswm_ad(input_data))
}

```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
rswm_ad_6_9 <- tab_content(honeycomb_grid_6_9)
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
rswm_ad_17_20 <- tab_content(honeycomb_grid_17_20)
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
rswm_ad_11_14 <- tab_content(honeycomb_grid_11_14)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
rswm_ad_16_19 <- tab_content(honeycomb_grid_16_19)
```
:::

#### 5.1.6. **Computing Global Spatial Autocorrelation Statistics**

In this subsection, we will employ two methods, namely **Moran's I** and **Geary's C**, to assess the following hypotheses:

-   **Null Hypothesis (H0)**: The observed spatial distribution of values is as likely as any other spatial arrangement, implying that the data is randomly distributed, devoid of discernible spatial patterns.

-   **Alternative Hypothesis (H1)**: The data exhibits a higher degree of spatial clustering than what would be expected by random chance alone.

#### 5.1.6.1. **Computing Global Moran' I**

In the code chunk below, `global_moran()` function is used to compute the Moran's I value. Different from **spdep** package, the output is a tibble data.frame.

```{r}
moranI <- function(input_data) {
  output <- global_moran(input_data$TOT_TRIPS,
                       input_data$nb,
                       input_data$wt)
  return(output)
}

tab_content <- function(input_data){
  return(moranI(input_data))
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
moranI_6_9 <- tab_content(rswm_ad_6_9)
glimpse(moranI_6_9)
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
moranI_17_20 <- tab_content(rswm_ad_17_20)
glimpse(moranI_17_20)
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
moranI_11_14 <- tab_content(rswm_ad_11_14)
glimpse(moranI_11_14)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
moranI_16_19 <- tab_content(rswm_ad_16_19)
glimpse(moranI_16_19)
```
:::

#### **5.1.6.1.1. Performing Global Moran's I test**

In general, Moran's I test will be performed instead of just computing the Moran's I statistics. With **sfdep** package, Moran's I test can be performed by using [`global_moran_test()`](https://sfdep.josiahparry.com/reference/global_moran_test.html) as shown in the code chunk below.

```{r}
tab_content <- function(input_data) {
  output <- global_moran_test(input_data$TOT_TRIPS,
                              input_data$nb,
                              input_data$wt)
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
tab_content(rswm_ad_6_9)
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
tab_content(rswm_ad_17_20)
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
tab_content(rswm_ad_11_14)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
tab_content(rswm_ad_16_19)
```
:::

#### 5.1.6.1.2. **Performing Global Moran'I permutation test**

In practice, monte carlo simulation should be used to perform the statistical test. For **sfdep**, it is supported by [`globel_moran_perm()`](https://sfdep.josiahparry.com/reference/global_moran_perm.html)

It is always a good practice to use `set.seed()` before performing simulation. This is to ensure that the computation is reproducible.

```{r}
set.seed(1234)
```

Next, `global_moran_perm()` is used to perform Monte Carlo simulation.

```{r}
tab_content <- function(input_data) {
  output <- global_moran_perm(input_data$TOT_TRIPS,
                              input_data$nb,
                              input_data$wt,
                              nsim = 99)
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
tab_content(rswm_ad_6_9)
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
tab_content(rswm_ad_17_20)
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
tab_content(rswm_ad_11_14)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
tab_content(rswm_ad_16_19)
```
:::

The report above show that the p-value is smaller than alpha value of 0.05. Hence, reject the null hypothesis that the spatial patterns spatial independent. Because the Moran's I statistics is greater than 0. We can infer the spatial distribution shows sign of clustering.

#### 5.1.6.2. **Computing Local Moran's I**

In this section, you will learn how to compute Local Moran's I of GDPPC at county level by using [`local_moran()`](https://sfdep.josiahparry.com/reference/local_moran.html) of sfdep package.

```{r}
lisa <- function(input_data) {
  output <- rswm_ad_6_9 %>%
    mutate(local_moran = local_moran(TOT_TRIPS, nb, wt, nsim = 99),
           .before = 1) %>%
    unnest(local_moran)
  return(output)
}

tab_content <- function(input_data){
  return(lisa(input_data))
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
lisa_6_9 <- tab_content(rswm_ad_6_9)
lisa_6_9
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
lisa_17_20 <- tab_content(rswm_ad_17_20)
lisa_17_20
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
lisa_11_14 <- tab_content(rswm_ad_11_14)
lisa_11_14 
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
lisa_16_19 <- tab_content(rswm_ad_16_19)
lisa_16_19
```
:::

The output of `local_moran()` is a sf data.frame containing the columns ii, eii, var_ii, z_ii, p_ii, p_ii_sim, and p_folded_sim.

-   ii: local moran statistic

-   eii: expectation of local moran statistic; for localmoran_permthe permutation sample means

-   var_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations

-   z_ii: standard deviate of local moran statistic; for localmoran_perm based on permutation sample means and standard deviations p_ii: p-value of local moran statistic using pnorm(); for localmoran_perm using standard deviatse based on permutation sample means and standard deviations p_ii_sim: For `localmoran_perm()`, `rank()` and `punif()` of observed statistic rank for \[0, 1\] p-values using `alternative=` -p_folded_sim: the simulation folded \[0, 0.5\] range ranked p-value (based on https://github.com/pysal/esda/blob/4a63e0b5df1e754b17b5f1205b cadcbecc5e061/esda/crand.py#L211-L213)

-   skewness: For `localmoran_perm`, the output of e1071::skewness() for the permutation samples underlying the standard deviates

-   kurtosis: For `localmoran_perm`, the output of e1071::kurtosis() for the permutation samples underlying the standard deviates.

#### 5.1.6.2.1. **Visualising local Moran's I**

In this code chunk below, tmap functions are used prepare a choropleth map by using value in the *ii* field.

```{r}
local_moranI <- function(input_data) {
  output <- tmap_mode("view") +
    tm_shape(input_data) +
    tm_fill("ii") +
    tm_borders(alpha = 0.5) +
    tm_view(set.zoom.limits = c(11, 20)) +
    tm_layout(main.title = "local Moran's I of Total Trips",
              main.title.size = 0.8)
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
local_moranI(lisa_6_9)
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
local_moranI(lisa_17_20)
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
local_moranI(lisa_11_14)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
local_moranI(lisa_16_19)
```
:::

#### 5.1.6.2.2. **Visualising p-value of local Moran's I**

In the code chunk below, tmap functions are used prepare a choropleth map by using value in the *p_ii_sim* field.

```{r}
local_moranI_pvalue <- function(input_data) {
  tmap_mode("view") +
    tm_shape(input_data) +
    tm_fill("p_ii_sim") +
    tm_borders(alpha = 0.5) +
    tm_layout(main.title = "p-value of local Moran's I",
              main.title.size = 0.8) 
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
local_moranI_pvalue(lisa_6_9)
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
local_moranI_pvalue(lisa_17_20)
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
local_moranI_pvalue(lisa_11_14)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
local_moranI_pvalue(lisa_16_19)
```
:::

#### 5.1.6.2.3. **Visualising local Moran's I and p-value**

For effective comparison, it will be better for us to plot both maps next to each other as shown below.

```{r}

map1 <- function(input_data) {
  tmap_mode("view") +
  tm_shape(input_data) +
    tm_fill("ii") +
    tm_borders(alpha = 0.5) +
    tm_view(set.zoom.limits = c(10, 15)) +
    tm_layout(main.title = "local Moran's I of Total Trips",
              main.title.size = 0.8)
}


map2 <- function(input_data) {
  tmap_mode("view") +
    tm_shape(input_data) +
    tm_fill(
      "p_ii_sim",
      breaks = c(0, 0.001, 0.01, 0.05, 1),
      labels = c("0.001", "0.01", "0.05", "Not sig")
    ) +
    tm_borders(alpha = 0.5) +
    tm_layout(main.title = "p-value of local Moran's I",
              main.title.size = 0.8)
}

```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
tmap_arrange(map1(lisa_6_9), map2(lisa_6_9), ncol = 2)
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
tmap_arrange(map1(lisa_17_20), map2(lisa_17_20), ncol = 2)
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
tmap_arrange(map1(lisa_11_14), map2(lisa_11_14), ncol = 2)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
tmap_arrange(map1(lisa_16_19), map2(lisa_16_19), ncol = 2)
```
:::

#### 5.1.6.2.4. **Visualising LISA map**

LISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters. In fact, LISA map is an interpreted map by combining local Moran's I of geographical areas and their respective p-values.

In lisa sf data.frame, we can find three fields contain the LISA categories. They are *mean*, *median* and *pysal*. In general, classification in *mean* will be used as shown in the code chunk below.

```{r}

visualize_lisa_map <- function(input_data) {
  lisa_sig <- input_data  %>%
    filter(p_ii_sim < 0.05)
  tmap_mode("view")
  tm_shape(input_data) +
    tm_polygons() +
    tm_borders(alpha = 0.5) +
    tm_shape(lisa_sig) +
    tm_fill("mean") +
    tm_borders(alpha = 0.4)
}  

```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
visualize_lisa_map(lisa_6_9)
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
visualize_lisa_map(lisa_17_20)
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
visualize_lisa_map(lisa_11_14)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
visualize_lisa_map(lisa_16_19)
```
:::

#### 5.1.6.2.5. **Interpretation of Results**

## 6. Conclusion

### 6.1. Future Work
