---
title: "Geospatial Analytics for Public Good"
author: "Su Sandi Cho Win"
date: "25 November 2023"
date-modified: "25 November 2023"
format: html
execute:
  echo: true
  eval: true
  warning: false
editor: visual
---

## 1. Overview

The digital transformation of urban infrastructure, including buses, taxis, mass transit, utilities, and roads, has generated vast datasets that serve as a foundation for tracking movement patterns over space and time. Pervasive technologies like GPS and RFID, widely adopted in vehicles, contribute to this trend. For example, smart cards and GPS devices collect route and ridership data on public buses. These extensive datasets likely contain valuable patterns that offer insights into observed phenomena. By deepening our understanding of human mobility in urban areas can improve urban management and provide vital information for both public and private urban transport providers, enhancing their decision-making and competitiveness.

In practical terms, the utilization of these extensive location-aware datasets has predominantly been confined to rudimentary tracking and mapping using Geographic Information System (GIS) applications. This limitation stems primarily from the conventional GIS's inherent lack of robust capabilities for effectively analyzing and modeling spatial and spatio-temporal data.

### 1.1. Objective

Exploratory Spatial Data Analysis (ESDA) represents a powerful approach with the capacity to tackle intricate societal challenges. Within the scope of this study, the objective is to employ suitable Local Indicators of Spatial Association (GLISA) and Emerging Hot Spot Analysis (EHSA) methodologies. This will enable us to unveil the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.

## 2. Getting Started

The following code chunk installs and loads **sf**, **sfdep**, **tmap**, **tidyverse, knitr, dplyr, hexbin** packages into R environment. [*pacman()*](https://cran.r-project.org/web/packages/pacman/readme/README.html) is a R package management tool.

```{r}
pacman::p_load(sf, sfdep, tmap, tidyverse, plotly, knitr, dplyr, hexbin)
```

## 3. Data Preparation

### 3.1. Data

The datasets used for this study are:

-   Master Plan 2019 Planning Sub-zone Geographic Information Systems (GIS) data set of URA from [data.gov.sg](https://beta.data.gov.sg/),

-   Passenger Volume by Origin Destination Bus Stops from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html),

-   Bus Stop Location dataset from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html)

### 3.2. Importing the Data into R Environment

#### **3.2.1. Importing Geospatial Data into R**

```{r}
busstop <- st_read(dsn = "data/geospatial",
                   layer = "BusStop") %>%
  st_transform(crs = 3414)
glimpse(busstop)
```

import *MPSZ-2019* downloaded from eLearn into RStudio and save it as a sf data frame called `mpsz`.

```{r}
mpsz <- st_read(dsn = "data/geospatial",
                   layer = "MPSZ-2019") %>%
  st_transform(crs = 3414)
```

#### **3.3.2. Importing Attribute Data into R**

Firstly, we will import the *Passenger Volume by Origin Destination Bus Stops* data set downloaded from LTA DataMall by using `read_csv()` of **readr** package.

```{r}
odbus <- read_csv("data/aspatial/origin_destination_bus_202308.csv")
glimpse(odbus)
```

```{r}
# Using tidyverse functions to convert these data values into factor data type.
odbus$ORIGIN_PT_CODE <- as.factor(odbus$ORIGIN_PT_CODE)
odbus$DESTINATION_PT_CODE <- as.factor(odbus$DESTINATION_PT_CODE) 
glimpse(odbus)
```

#### 

#### 3.3.3. **Extracting the study data**

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
origin6_9 <- odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 6 &
           TIME_PER_HOUR <= 9) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

kable(head(origin6_9))
```

We will save the output in rds format for future use.

```{r}
write_rds(origin6_9, "data/rds/origin6_9.rds")
```

The follwing code chunk will be used to import the saved origin6_9.rds into R environment.

```{r}
origin6_9 <- read_rds("data/rds/origin6_9.rds")
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
origin17_20 <- odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 17 &
           TIME_PER_HOUR <= 20) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

kable(head(origin17_20))
```

We will save the output in rds format for future use.

```{r}
write_rds(origin17_20, "data/rds/origin17_20.rds")
```

The following code chunk will be used to import the saved origin17_20. rds into R environment.

```{r}
origin17_20 <- read_rds("data/rds/origin17_20.rds")
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
origin11_14 <- odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 11 &
           TIME_PER_HOUR <= 14) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

kable(head(origin11_14))
```

We will save the output in rds format for future use.

```{r}
write_rds(origin11_14, "data/rds/origin11_14.rds")
```

The following code chunk will be used to import the saved origin11_14.rds into R environment.

```{r}
origin11_14 <- read_rds("data/rds/origin11_14.rds")
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
origin16_19 <- odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 16 &
           TIME_PER_HOUR <= 19) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

kable(head(origin16_19))
```

We will save the output in rds format for future use.

```{r}
write_rds(origin16_19, "data/rds/origin16_19.rds")
```

The following code chunk will be used to import the saved origin16_19.rds into R environment.

```{r}
origin16_19 <- read_rds("data/rds/origin16_19.rds")
```
:::

### **3.4. Data Wrangling - Geospatial Data**

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
origin6_9_busstop <- left_join(busstop, origin6_9,
            by = c("BUS_STOP_N" = "ORIGIN_PT_CODE"))
origin6_9_busstop
```

```{r}
duplicate <- origin6_9_busstop %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
origin17_20_busstop <- left_join(busstop, origin17_20,
            by = c("BUS_STOP_N" = "ORIGIN_PT_CODE"))
origin17_20_busstop
```

```{r}
duplicate <- origin17_20_busstop %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
origin11_14_busstop <- left_join(busstop, origin11_14,
            by = c("BUS_STOP_N" = "ORIGIN_PT_CODE"))
origin11_14_busstop
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
origin16_19_busstop <- left_join(busstop, origin16_19,
            by = c("BUS_STOP_N" = "ORIGIN_PT_CODE"))
origin16_19_busstop
```
:::

3.5. Visualizing

Preparing a reusable function to process input data

```{r}
honeycomb_grid <- function(input_data) {
  area_honeycomb_grid_current <-
    st_make_grid(input_data, c(500), what = "polygon", square = FALSE)
  
  # To sf and add grid ID
  honeycomb_grid_sf_current <- st_sf(area_honeycomb_grid_current) %>%
    # add grid ID
    mutate(grid_id = 1:length(lengths(area_honeycomb_grid_current)))
  
  # Perform spatial intersection to count points within each hexagon
  point_counts_current <-
    st_intersection(honeycomb_grid_sf_current, input_data) %>%
    group_by(grid_id) %>%
    summarize(TOT_TRIPS = sum(TRIPS, na.rm = TRUE))  # Replace 'TRIPS' with your column name
  
  # Merge the point counts back into the honeycomb grid
  honeycomb_grid_sf_current <- honeycomb_grid_sf_current %>%
    st_join(point_counts_current, by = "grid_id")
  
  # remove grid without value of 0 (i.e. no points in side that grid)
  honeycomb_count_current <-
    filter(honeycomb_grid_sf_current, TOT_TRIPS > 0)
  
  return(honeycomb_count_current)
}
```

Preparing a reusable function to plot the honeycomb map

```{r}
draw_honeycomb_map <- function(input_data) {
  map_output <- tm_shape(input_data) +
    tm_fill(
      breaks = c(0, 2000, 5000, 10000, 20000, 500000),
      labels = c("0 to 1999", "2000 to 4999", "5000 to 9999", "10000 to 19999", "20000 to 499999"),
      col = "TOT_TRIPS",
      palette = "Reds",
      style = "fixed",
      title = "Number of Trips",
      id = "grid_id",
      showNA = FALSE,
      alpha = 0.6,
      popup.vars = c(
        "Number of trips: " = "TOT_TRIPS"
      ),
      popup.format = list(
        TOT_TRIPS = list(format = "f", digits = 0)
      )
    ) +
    tm_borders(col = "grey40", lwd = 0.7)
  
  return (map_output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
honeycomb_grid(origin6_9_busstop) %>% 
  draw_honeycomb_map
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
honeycomb_grid(origin17_20_busstop) %>% 
  draw_honeycomb_map
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
honeycomb_grid(origin11_14_busstop) %>% 
  draw_honeycomb_map
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
honeycomb_grid(origin16_19_busstop) %>% 
  draw_honeycomb_map
```
:::

## 4. Exploratory Data Analysis

## 5. Exploratory Spatial Data Analysis

Exploratory Spatial Data Analysis (ESDA) is an expansion of the Exploratory Data Analysis concept, encompassing descriptive methods aimed at unveiling the spatial distribution of data and detecting anomalies.

In this segment, we will delve into both global and local spatial autocorrelation. The global perspective highlights overarching trends, whereas the local analysis empowers us to pinpoint regions of heightened and reduced values within the dataset.

### 5.1. **Global Spatial Autocorrelation**

In accordance with Tobler's First Law of Geography, it is acknowledged that "all things are interconnected, but objects in close proximity exhibit stronger connections than those farther apart.".

In this subsection, we will delve into the calculation of global spatial autocorrelation statistics and the assessment of spatial randomness on a global scale. The objective of these analyses is to gain insights into the distribution of passenger volume by Origin Destination Bus Stops across Singapore, assessing whether they are uniformly dispersed or not.

#### 5.1.1. **Spatial Weights Matrix**

To perform global spatial autocorrelation analysis, our initial step involves the creation of spatial weights for Singapore to establish the spatial neighborhood, a fundamental element for subsequent spatial analysis. There are two prevalent approaches for generating spatial weights: contiguity-based and distance-based methods.

In contiguity-based methods, neighboring areas are determined based on shared boundaries, with variations in how these boundaries are defined in different methods. Rook contiguity, for instance, considers neighbors that share a common edge. Conversely, Queen contiguity deems neighbors as those who share either a common vertex or edge. Consequently, Queen contiguity is more inclusive compared to Rook contiguity.

The distinctions between these two approaches are elucidated in the accompanying illustration.

![](Rooks-vs-Queens-Contiguity.png){fig-align="center"}

In distance-based contiguity, there are two distinct weighting schemes: fixed weighting and adaptive weighting. The former approach deems two regions as neighbors if they fall within a predetermined distance of each other. In the latter scheme, every region is assigned an equal number of neighbors, with the specific number of neighbors predetermined. For instance, if we set k = 8 neighbors, it will categorize the nearest eight regions as neighbors for each.

The choice of which spatial weights method to employ hinges on the geographical characteristics of the area under consideration. In cases where the geographical area comprises numerous isolated islands, opting for a contiguity-based matrix might lead to many regions lacking neighbors. Similarly, if the features (polygons) exhibit a wide range of sizes, encompassing both very large and relatively smaller ones, the use of a contiguity-based matrix may result in larger features having a disproportionate number of neighbors. This could potentially introduce a smoothing effect due to the higher number of neighbors for larger features, potentially skewing the analytical outcomes.

#### 5.1.2. **Contiguity-based Spatial Weights**

In broad terms, spatial weights can be categorized into two main types: contiguity weights and distance-based weights. This section will focus on the creation of contiguity spatial weights using the **sfdep** package.

Deriving contiguity spatial weights involves two key steps:

1.  Identifying the contiguity neighbor list using the [`st_contiguity()`](https://sfdep.josiahparry.com/reference/st_contiguity.html) function from the **sfdep** package.

2.  Generating the contiguity spatial weights using the [`st_weights()`](https://sfdep.josiahparry.com/reference/st_weights.html) function, also part of the **sfdep** package.

Throughout this section, we will delve into the process of obtaining the contiguity neighbor list and the contiguity spatial weights separately. Subsequently, we will explore how to seamlessly combine both steps into a single procedure.

#### 5.1.2.1. **Contiguity-based (Queen) Spatial Weight Contiguity**

The following code chunk utilizes [`st_contiguity()`](https://sfdep.josiahparry.com/reference/st_contiguity.html) to derive a contiguity neighbour list by using Queen's method.

```{r}
nb_queen <- mpsz %>% 
  mutate(nb = st_contiguity(geometry),
         .before = 1)
```

```{r}
summary(nb_queen$nb)
```

The summary report above shows that there are 332 area units in Singapore. The most connected area units have 17 neighbors. There are two area units with only one neighbor.

```{r}
nb_queen
```

The print shows that polygon 1 has five neighbors. They are polygons number 6, 36, 52, 70 and 91.

```{r}
kable(head(nb_queen,
           n=10))
```

#### 5.1.2.2. **Contiguity-based (Rook) Spatial Weight Contiguity**

```{r}
nb_rook <- mpsz %>% 
  mutate(nb = st_contiguity(geometry,
                            queen = FALSE),
         .before = 1)
```

```{r}
summary(nb_rook$nb)
```

The summary report above shows that there are 332 area units in Singapore. The most connected area units have 16 neighbors. There are two area units with only one neighbor.

```{r}
nb_rook
```

The print shows that polygon 1 has five neighbors. They are polygons number 6, 36, 52, 70 and 91.

```{r}
kable(head(nb_rook,
           n=10))
```

#### 5.1.3. **Distance-based Contiguity Weight Matrix**

There are three commonly employed types of distance-based spatial weights:

1.  Fixed distance weights

2.  Adaptive distance weights

3.  Inverse distance weights (IDW)

#### 5.1.3.1. **Fixed Distance Weight Matrix**

```{r}
geo <- sf::st_geometry(mpsz)
nb <- st_knn(geo, longlat = TRUE)
dists <- unlist(st_nb_dists(geo, nb))
```

Now, we will go ahead to derive summary statistics of the nearest neighbor distances vector (i.e. dists) by usimgn the following code chunk.

```{r}
summary(dists)
```

The summary statistics report above shows that the maximum nearest neighbor distance is 10130.8km. By using a threshold value of 10000km will ensure that each area will have at least one neighbor.

Now we will go ahead to compute the fixed distance weights by using the code chunk below.

```{r}
wm_fd <- mpsz %>%
  mutate(nb = st_dist_band(geometry,
                           upper = 10200),
               wt = st_weights(nb),
               .before = 1)
```

```{r}
longitude <- map_dbl(mpsz$geometry, ~st_centroid(.x)[[1]])
latitude <- map_dbl(mpsz$geometry, ~st_centroid(.x)[[2]])
coords <- c(longitude, latitude)
head(coords, 5)
```

```{r}
par(mfrow = c(1,2))
plot(mpsz$geometry, border = "lightgrey",main="1st nearest neighbours" )
plot(dists, coords, add = TRUE, col = "red", length = 0.88, )

plot(mpsz$geometry, border = "lightgrey", main = "Distance Link")
plot(wm_fd, coords, add = TRUE, pch = 19, cex = 0.6)
```

#### 5.1.3.2. **Adaptive Distance-based Weight Matrix**

To address the problem of an uneven distribution of neighbors in a fixed distance weight matrix, we can directly control the number of neighbors by employing the k-nearest neighbors approach, as demonstrated in the following code snippet.

As a general guideline, we will set k = 8, meaning that each region will be assigned exactly 8 neighbors.

```{r}
wm_ad <- mpsz %>% 
  mutate(nb = st_knn(geometry,
                     k=8),
         wt = st_weights(nb),
               .before = 1)
```

#### 5.1.3.3. Inverse distance weights (IDW)

The following code chunk utilizes:

-   [`st_contiguity()`](https://sfdep.josiahparry.com/reference/st_contiguity.html) to identify the neighbors by using contiguity criteria. The output is a list of neighbors (i.e. nb).

-   [`st_inverse_distance()`](https://sfdep.josiahparry.com/reference/st_inverse_distance.html) to calculate inverse distance weights of neighbors on the nb list.

```{r}
wm_idw <- mpsz %>%
  mutate(nb = st_contiguity(geometry),
         wts = st_inverse_distance(nb, geometry,
                                   scale = 1,
                                   alpha = 1),
         .before = 1)
```

#### 5.1.4. **Which spatial weight matrix to use?**

The choice of a spatial weight matrix to use depends on the geographical area under investigation and the research objectives. In our specific context, when considering options between contiguity-based and distance-based spatial weight matrices, we are inclined towards the use of distance-based matrices. Within the category of distance-based matrices, we will specifically opt for the adaptive distance-based spatial weight matrix for our forthcoming analyses.

Here are the summarized reasons for our choice:

1.  Nigeria comprises 761 LGAs with varying sizes. Therefore, a contiguity-based matrix would result in larger LGAs having more neighbors and smaller LGAs having fewer neighbors, potentially distorting our analysis. Thus, distance-based methods are favored.

2.  As previously mentioned, the fixed distance-based method has the drawback that some regions would have only one neighbor, while the average number of neighbors for regions is 23. Statistical tests for regions with only one neighbor may not be valid.

Taking these factors into account, our selection will be the adaptive distance-based spatial weight matrix.

#### 5.1.5. **Row-Standardized Weights Matrix**

With the weight matrix chosen, the next step is to assign weights to each neighboring polygon. In this process, every neighboring polygon will be given equal weight (style="W") by allocating the fraction 1/(number of neighbors) to each adjacent area. This corresponds to what is referred to as a row-standardized matrix, where each row in the matrix totals to 1.

```{r}
wm_q <- mpsz %>%
  mutate(nb = st_contiguity(geometry),
         wt = st_weights(nb,
                         style = "W"),
         .before = 1) 
```

#### 5.1.6. **Computing Global Spatial Autocorrelation Statistics**

In this subsection, we will employ two methods, namely **Moran's I** and **Geary's C**, to assess the following hypotheses:

-   **Null Hypothesis (H0)**: The observed spatial distribution of values is as likely as any other spatial arrangement, implying that the data is randomly distributed, devoid of discernible spatial patterns.

-   **Alternative Hypothesis (H1)**: The data exhibits a higher degree of spatial clustering than what would be expected by random chance alone.

#### 5.1.6.1. **Computing Global Moran\' I**

In the code chunk below, `global_moran()` function is used to compute the Moran\'s I value. Different from **spdep** package, the output is a tibble data.frame.

```{r}
moranI <- global_moran(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt)
glimpse(moranI)
```

#### **5.1.6.1.1. Performing Global Moran\'s I test**

In general, Moran\'s I test will be performed instead of just computing the Moran\'s I statistics. With sfdep package, Moran\'s I test can be performed by using [`global_moran_test()`](https://sfdep.josiahparry.com/reference/global_moran_test.html) as shown in the code chunk below.

```{r}
global_moran_test(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt)
```

#### 5.1.6.1.2. **Performing Global Moran\'I permutation test**

In practice, monte carlo simulation should be used to perform the statistical test. For **sfdep**, it is supported by [`globel_moran_perm()`](https://sfdep.josiahparry.com/reference/global_moran_perm.html)

It is always a good practice to use `set.seed()` before performing simulation. This is to ensure that the computation is reproducible.

```{r}
set.seed(1234)
```

Next, `global_moran_perm()` is used to perform Monte Carlo simulation.

```{r}
global_moran_perm(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt,
                  nsim = 99)
```

The report above show that the p-value is smaller than alpha value of 0.05. Hence, reject the null hypothesis that the spatial patterns spatial independent. Because the Moran\'s I statistics is greater than 0. We can infer the spatial distribution shows sign of clustering.

#### 5.1.6.2. **Computing Local Moran\'s I**

In this section, you will learn how to compute Local Moran\'s I of GDPPC at county level by using [`local_moran()`](https://sfdep.josiahparry.com/reference/local_moran.html) of sfdep package.

```{r}
lisa <- wm_q %>% 
  mutate(local_moran = local_moran(
    GDPPC, nb, wt, nsim = 99),
         .before = 1) %>%
  unnest(local_moran)
```

The output of `local_moran()` is a sf data.frame containing the columns ii, eii, var_ii, z_ii, p_ii, p_ii_sim, and p_folded_sim.

-   ii: local moran statistic

-   eii: expectation of local moran statistic; for localmoran_permthe permutation sample means

-   var_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations

-   z_ii: standard deviate of local moran statistic; for localmoran_perm based on permutation sample means and standard deviations p_ii: p-value of local moran statistic using pnorm(); for localmoran_perm using standard deviatse based on permutation sample means and standard deviations p_ii_sim: For `localmoran_perm()`, `rank()` and `punif()` of observed statistic rank for \[0, 1\] p-values using `alternative=` -p_folded_sim: the simulation folded \[0, 0.5\] range ranked p-value (based on https://github.com/pysal/esda/blob/4a63e0b5df1e754b17b5f1205b cadcbecc5e061/esda/crand.py#L211-L213)

-   skewness: For `localmoran_perm`, the output of e1071::skewness() for the permutation samples underlying the standard deviates

-   kurtosis: For `localmoran_perm`, the output of e1071::kurtosis() for the permutation samples underlying the standard deviates.

#### 5.1.6.2.1. **Visualising local Moran\'s I**

In this code chunk below, tmap functions are used prepare a choropleth map by using value in the *ii* field.

```{r}
tmap_mode("plot")
tm_shape(lisa) +
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "local Moran's I of GDPPC",
            main.title.size = 0.8)
```

#### 5.1.6.2.2. **Visualising p-value of local Moran\'s I**

In the code chunk below, tmap functions are used prepare a choropleth map by using value in the *p_ii_sim* field.

```{r}
tmap_mode("plot")
tm_shape(lisa) +
  tm_fill("p_ii_sim") + 
  tm_borders(alpha = 0.5) +
   tm_layout(main.title = "p-value of local Moran's I",
            main.title.size = 0.8)
```

#### 5.1.6.2.3. **Visuaising local Moran\'s I and p-value**

For effective comparison, it will be better for us to plot both maps next to each other as shown below.

```{r}
tmap_mode("plot")
map1 <- tm_shape(lisa) +
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "local Moran's I of GDPPC",
            main.title.size = 0.8)

map2 <- tm_shape(lisa) +
  tm_fill("p_ii",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
              labels = c("0.001", "0.01", "0.05", "Not sig")) + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "p-value of local Moran's I",
            main.title.size = 0.8)

tmap_arrange(map1, map2, ncol = 2)
```

#### 5.1.6.2.4. **Visualising LISA map**

LISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters. In fact, LISA map is an interpreted map by combining local Moran\'s I of geographical areas and their respective p-values.

In lisa sf data.frame, we can find three fields contain the LISA categories. They are *mean*, *median* and *pysal*. In general, classification in *mean* will be used as shown in the code chunk below.

\`\`\`{r}

#### 5.1.6.3. **Geary's C**

#### 5.1.7. **Spatial Correlogram**

#### 5.1.7.1. **Moran's I Correlogram**

#### 5.1.7.2. **Geary's C Correlogram**

#### 5.2. **Local Spatial Autocorrelation Statistics**

#### 5.2.1. **Cluster and Outlier Analysis**

#### 5.2.1.1. **Local Moran's I**

#### 5.2.1.2. **Anselin's Moran Scatterplot**

#### 5.2.1.3. **LISA Cluster Maps**

#### 5.2.1.4. **Interpretation of Results**

#### 5.2.2. **Hot Spot Area Analysis**

#### 5.2.2.1. **Getis and Ord's G-Statistics**

#### 5.2.2.2. **Interpretation of Results**

## 6. Conclusion

### 6.1. Future Work
