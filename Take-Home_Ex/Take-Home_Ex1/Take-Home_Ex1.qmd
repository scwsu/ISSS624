---
title: "Geospatial Analytics for Public Good"
author: "Su Sandi Cho Win"
date: "25 November 2023"
date-modified: "03 December 2023"
format: html
execute:
  echo: true
  eval: true
  warning: false
editor: visual
---

## 1. Overview

The digital transformation of urban infrastructure, including buses, taxis, mass transit, utilities, and roads, has generated vast datasets that serve as a foundation for tracking movement patterns over space and time. Pervasive technologies like GPS and RFID, widely adopted in vehicles, contribute to this trend. For example, smart cards and GPS devices collect route and ridership data on public buses. These extensive datasets likely contain valuable patterns that offer insights into observed phenomena. By deepening our understanding of human mobility in urban areas can improve urban management and provide vital information for both public and private urban transport providers, enhancing their decision-making and competitiveness.

In practical terms, the utilization of these extensive location-aware datasets has predominantly been confined to rudimentary tracking and mapping using Geographic Information System (GIS) applications. This limitation stems primarily from the conventional GIS's inherent lack of robust capabilities for effectively analyzing and modeling spatial and spatio-temporal data.

### 1.1. Objective

Exploratory Spatial Data Analysis (ESDA) represents a powerful approach with the capacity to tackle intricate societal challenges. Within the scope of this study, the objective is to employ suitable Local Indicators of Spatial Association (GLISA) and Emerging Hot Spot Analysis (EHSA) methodologies. This will enable us to unveil the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.

## 2. Getting Started

The following code chunk installs and loads **sf**, **sfdep**, **tmap**, **tidyverse, knitr, hexbin** packages into R environment. [`pacman()`](https://cran.r-project.org/web/packages/pacman/readme/README.html)is a R package management tool.

```{r}
pacman::p_load(sf, sfdep, tmap, tidyverse, plotly, knitr, hexbin)
```

## 3. Data Preparation

### 3.1. Data

The datasets used for this study are:

-   Master Plan 2019 Planning Sub-zone Geographic Information Systems (GIS) data set of URA from [data.gov.sg](https://beta.data.gov.sg/),

-   Passenger Volume by Origin Destination Bus Stops from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html),

-   Bus Stop Location dataset from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html).

### 3.2. Importing the Data into R Environment

#### **3.2.1. Importing Geospatial Data into R**

The following code chunk utilizes the `st_read()` function from the **sf** package to import:

-   *BusStop* shapefile into R, creating a simple feature data frame named `busstop`, and

-   *MPSZ-2019* shapefile into R and save it as a sf data frame called `mpsz`.

```{r}
busstop <- st_read(dsn = "data/geospatial",
                   layer = "BusStop") %>%
  st_transform(crs = 3414)
glimpse(busstop)
```

```{r}
mpsz <- st_read(dsn = "data/geospatial",
                   layer = "MPSZ-2019") %>%
  st_transform(crs = 3414)
```

#### **3.3.2. Importing Attribute Data into R**

To begin, we utilize the `read_csv()` function from the **readr** package to import the *Passenger Volume by Origin Destination Bus Stops* dataset for the month of October 2023, downloaded from LTADataMall and name it as `odbus`.

```{r}
odbus <- read_csv("data/aspatial/origin_destination_bus_202310.csv")
glimpse(odbus)
```

The following code chunk converts the *ORIGIN_PT_CODE* and *DESTINATION_PT_CODE* columns in the `odbus` data frame into factor data types, making them suitable for further analysis. It then uses `glimpse()` function to verify if the conversion is successful.

```{r}
# Using tidyverse functions to convert these data values into factor data type.
odbus$ORIGIN_PT_CODE <- as.factor(odbus$ORIGIN_PT_CODE)
odbus$DESTINATION_PT_CODE <- as.factor(odbus$DESTINATION_PT_CODE) 
glimpse(odbus)
```

#### 3.3.3. **Extracting the study data**

For the purpose of this project, we will study data related to commuting patterns on:

-   weekdays during the busy morning rush hours (6am \~ 9am),

-   weekdays during the busy afternoon rush hours (5pm \~ 8pm),

-   weekends/holidays during the busy morning rush hours (11am \~ 2pm),

-   weekends/holidays during the busy evening rush hours (4pm \~ 7pm).

::: panel-tabset
## Weekday Morning Peak 6am to 9am

The following code chunk creates a new data frame called `wdmp` by filtering the `odbus` data frame to retain only weekday data between **6AM** and **9AM**, grouping it by *ORIGIN_PT_CODE*, and then calculating the total number of trips for each origin point. Afterward, it displays the first few rows of the `wdmp` data frame in a tabular format using the `kable()` function for visual inspection or reporting purposes.

```{r}
wdmp <- odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 6 &
           TIME_PER_HOUR <= 9) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

kable(head(wdmp))
```

The following code chunk is used to save the `wdmp` data object in rds (R Data Serialization) format file named *wdmp.rds* in the **data/rds** directory.

```{r}
write_rds(wdmp, "data/rds/wdmp.rds")
```

The following code chunk is used to import the saved *wdmp.rds* into R environment.

```{r}
wdmp <- read_rds("data/rds/wdmp.rds")
```

## Weekday Afternoon Peak 5pm to 8pm

The following code chunk creates a new data frame called `wdap` by filtering the `odbus` data frame to retain only weekday data between **5PM** and **8PM**, grouping it by *ORIGIN_PT_CODE*, and then calculating the total number of trips for each origin point. Afterward, it displays the first few rows of the `wdap` data frame in a tabular format using the `kable()` function for visual inspection or reporting purposes.

```{r}
wdap <- odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 17 &
           TIME_PER_HOUR <= 20) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

kable(head(wdap))
```

The following code chunk is used to save the `wdap` data object in rds (R Data Serialization) format file named *wdap.rds* in the **data/rds** directory.

```{r}
write_rds(wdap, "data/rds/wdap.rds")
```

The following code chunk is used to import the saved *wdap.rds* into R environment.

```{r}
wdap <- read_rds("data/rds/wdap.rds")
```

## Weekends/Holiday Morning Peak 11am to 2pm

The following code chunk creates a new data frame called `wehmp` by filtering the `odbus` data frame to retain only weekend/holiday data between **11AM** and **2PM**, grouping it by *ORIGIN_PT_CODE*, and then calculating the total number of trips for each origin point. Afterward, it displays the first few rows of the `wehmp` data frame in a tabular format using the `kable()` function for visual inspection or reporting purposes.

```{r}
wehmp <- odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 11 &
           TIME_PER_HOUR <= 14) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

kable(head(wehmp))
```

The following code chunk is used to save the `wehmp` data object in rds (R Data Serialization) format file named *wehmp.rds* in the **data/rds** directory.

```{r}
write_rds(wehmp, "data/rds/wehmp.rds")
```

The following code chunk is used to import the saved *wehmp.rds* into R environment.

```{r}
wehmp <- read_rds("data/rds/wehmp.rds")
```

## Weekends/Holiday Evening Peak 4pm to 7pm

The following code chunk creates a new data frame called `wehep` by filtering the `odbus` data frame to retain only weekend/holiday data between **4PM** and **7PM**, grouping it by *ORIGIN_PT_CODE*, and then calculating the total number of trips for each origin point. Afterward, it displays the first few rows of the `wehep` data frame in a tabular format using the `kable()` function for visual inspection or reporting purposes.

```{r}
wehep <- odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 16 &
           TIME_PER_HOUR <= 19) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

kable(head(wehep))
```

The following code chunk is used to save the `wehep` data object in rds (R Data Serialization) format file named *wehep.rds* in the **data/rds** directory.

```{r}
write_rds(wehep, "data/rds/wehep.rds")
```

The following code chunk is used to import the saved *wehep.rds* into R environment.

```{r}
wehep <- read_rds("data/rds/wehep.rds")
```
:::

### **3.4. Data Wrangling - Geospatial Data**

Next, we prepare a reusable function to process input each study data.

The following function, `join_data` performs a left join between the `busstop` dataset and the input data using the *BUS_STOP_N* and *ORIGIN_PT_CODE* columns as matching keys and returns the resulting merged dataset.

```{r}
join_data <- function(input_data) {
  output <- left_join(busstop, input_data,
            by = c("BUS_STOP_N" = "ORIGIN_PT_CODE"))
  return(output)
}
```

The following function, `find_duplicate` takes an input dataset, groups it by all columns, filters and retains rows with duplicate values, and then ungroups the dataset, effectively identifying and returning duplicate rows within the input data.

```{r}
find_duplicate <- function(input_data) {
  input_data %>%
    group_by_all() %>%
    filter(n() > 1) %>%
    ungroup()
  
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

The following code chunk combines the `wdmp` dataset with the `busstop` dataset using `join_data` function, resulting in a merged dataset named `wdmp_busstop`.

```{r}
wdmp_busstop <- join_data(wdmp)
wdmp_busstop 
```

The following code chunk utilizes `find_duplicate` function to find duplicate rows in the data.

```{r}
find_duplicate_wdmp_busstop <- find_duplicate(wdmp_busstop)
```

## Weekday Afternoon Peak 5pm to 8pm

The following code chunk combines the `wdap` dataset with the `busstop` dataset using `join_data` function, resulting in a merged dataset named `wdmp_busstop`.

```{r}
wdap_busstop <- join_data(wdap)
wdap_busstop 
```

The following code chunk utilizes `find_duplicate` function to find duplicate rows in the data.

```{r}
find_duplicate_wdap_busstop <- find_duplicate(wdap_busstop)
```

## Weekends/Holiday Morning Peak 11am to 2pm

The following code chunk combines the `wehmp` dataset with the `busstop` dataset using `join_data` function, resulting in a merged dataset named `wehmp_busstop`.

```{r}
wehmp_busstop <- join_data(wehmp)
wehmp_busstop 
```

The following code chunk utilizes `find_duplicate` function to find duplicate rows in the data.

```{r}
find_duplicate_wehmp_busstop <- find_duplicate(wehmp_busstop)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

The following code chunk combines the `wehep` dataset with the `busstop` dataset using `join_data` function, resulting in a merged dataset named `wehep_busstop`.

```{r}
wehep_busstop <- join_data(wehep)
wehep_busstop 
```

The following code chunk utilizes `find_duplicate` function to find duplicate rows in the data.

```{r}
find_duplicate_wehep_busstop <- find_duplicate(wehep_busstop)
```
:::

## 4. Exploratory Data Analysis

### 4.1. Creating a map with total number of trips by origin at the hexagon level

Next, we prepare a reusable function to compute the passenger trips generated by origin at the hexagon level.

The following function, `honeycomb_grid` is used to generate a **hexagonal grid overlay** on a spatial dataset, counts the points within each hexagon, and returns a modified grid with the counts, excluding grids with no points (count equal to 0).

```{r}
honeycomb_grid <- function(input_data) {
  area_honeycomb_grid_current <-
    st_make_grid(input_data, c(500), what = "polygon", square = FALSE)
  
  # To sf and add grid ID
  honeycomb_grid_sf_current <- st_sf(area_honeycomb_grid_current) %>%
    # add grid ID
    mutate(grid_id = 1:length(lengths(area_honeycomb_grid_current)))
  
  # Perform spatial intersection to count points within each hexagon
  point_counts_current <-
    st_intersection(honeycomb_grid_sf_current, input_data) %>%
    group_by(grid_id) %>%
    summarize(TOT_TRIPS = sum(TRIPS, na.rm = TRUE),
              BUS_STOP_N = paste(BUS_STOP_N, collapse = ", "))  
  
  # Merge the point counts back into the honeycomb grid
  honeycomb_grid_sf_current <- honeycomb_grid_sf_current %>%
    st_join(point_counts_current, by = "grid_id")
  
  # remove grid without value of 0 (i.e. no points in side that grid)
  honeycomb_count_current <-
    filter(honeycomb_grid_sf_current, TOT_TRIPS > 0) 
  
  return(honeycomb_count_current)
}
```

The following function, `draw_honeycomb_map` is used to create a thematic map from the input data, displaying the *TOT_TRIPS* values in a color-coded style based on quantiles, with additional features like borders, tooltips, and opacity adjustments, and returns the map as an output.

```{r}
draw_honeycomb_map <- function(input_data) {
  map_output <- tm_shape(input_data) +
    tm_fill(
      col = "TOT_TRIPS",
      palette = "Reds",
      style = "quantile",
      title = "Number of Trips",
      id = "grid_id",
      showNA = FALSE,
      alpha = 0.6,
      popup.vars = c(
        "Number of trips: " = "TOT_TRIPS",
        "Bus stop number:" = "BUS_STOP_N"
        
      ),
      popup.format = list(
        TOT_TRIPS = list(format = "f", digits = 0),
        BUS_STOP_N = list(format = "f", digits = 0)
      )
    ) +
    tm_borders(col = "grey40", lwd = 0.7) +
    tmap_mode("view")

  return (map_output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

The following code chunk first generates a hexagonal grid with trip counts using the `honeycomb_grid` function applied to the `wdmp_busstop` dataset and then creates a thematic map displaying trip counts based on quantiles using the `draw_honeycomb_map` function with the generated grid data.

```{r}
honeycombgrid_wdmp <- honeycomb_grid(wdmp_busstop)
draw_honeycomb_map(honeycombgrid_wdmp)
```

## Weekday Afternoon Peak 5pm to 8pm

The following code chunk first generates a hexagonal grid with trip counts using the `honeycomb_grid` function applied to the `wdap_busstop` dataset and then creates a thematic map displaying trip counts based on quantiles using the `draw_honeycomb_map` function with the generated grid data.

```{r}
honeycombgrid_wdap <- honeycomb_grid(wdap_busstop)
draw_honeycomb_map(honeycombgrid_wdap)
```

## Weekends/Holiday Morning Peak 11am to 2pm

The following code chunk first generates a hexagonal grid with trip counts using the `honeycomb_grid` function applied to the `wehmp_busstop` dataset and then creates a thematic map displaying trip counts based on quantiles using the `draw_honeycomb_map` function with the generated grid data.

```{r}
honeycombgrid_wehmp <- honeycomb_grid(wehmp_busstop)
draw_honeycomb_map(honeycombgrid_wehmp)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

The following code chunk first generates a hexagonal grid with trip counts using the `honeycomb_grid` function applied to the `wehep_busstop` dataset and then creates a thematic map displaying trip counts based on quantiles using the `draw_honeycomb_map` function with the generated grid data.

```{r}
honeycombgrid_wehep <- honeycomb_grid(wehep_busstop)
draw_honeycomb_map(honeycombgrid_wehep)
```
:::

### 4.2. Removing irrelevant data

From the maps above, we note that there are a few bus stops (BUS_STOP_N = 46211, 46219, 46239) that are located outside Singapore. Hence, we prepare a reusable function to remove those from our study data.

```{r}
bus_stop_numbers_to_exclude <- c(46211, 46219, 46239)
busstop_filtered <- function(input_data) {
  output <- input_data %>%
  filter(!BUS_STOP_N %in% bus_stop_numbers_to_exclude)
  return(output)
}
```

Next, we prepare a reusable function to verify if the excluded bus stop numbers are still present in the data after filtering.

```{r}
# Function to check if excluded bus stop numbers are present in the data
check_excluded_bus_stops <- function(input_data, bus_stop_numbers_to_exclude) {
  # Find the bus stop numbers that are in the data after filtering
  remaining_bus_stops <- input_data$BUS_STOP_N
  
  # Check if any of the excluded bus stop numbers are still in the data
  still_in_data <- any(bus_stop_numbers_to_exclude %in% remaining_bus_stops)
  
  if (still_in_data) {
    message("Excluded bus stop numbers are still in the data.")
  } else {
    message("Excluded bus stop numbers are no longer in the data.")
  }
  
  return(still_in_data)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

The following code chunk utilizes `busstop_filtered` function and name the filtered dataset as `wdmp_busstop_filtered`. After filtering, `check_excluded_bus_stop` function is used to determine if any of the excluded bus stop numbers are still present in `wdmp_busstop_filtered`.

```{r}
wdmp_busstop_filtered <- busstop_filtered(wdmp_busstop)
check_excluded_bus_stops(wdmp_busstop_filtered, bus_stop_numbers_to_exclude)
```

Next, we generate a new `honeycombgrid_wdmp_filtered` data based on `wdmp_busstop_filtered` data by using `honeycomb_grid` function created in Section 4.1.

```{r}
honeycombgrid_wdmp_filtered <- honeycomb_grid(wdmp_busstop_filtered)
```

## Weekday Afternoon Peak 5pm to 8pm

The following code chunk utilizes `busstop_filtered` function and name the filtered dataset as `wdap_busstop_filtered`. After filtering, `check_excluded_bus_stop` function is used to determine if any of the excluded bus stop numbers are still present in `wdap_busstop_filtered`.

```{r}
wdap_busstop_filtered <- busstop_filtered(wdap_busstop)
check_excluded_bus_stops(wdap_busstop_filtered, bus_stop_numbers_to_exclude)
```

Next, we generate a new `honeycombgrid_wdap_filtered` data based on `wdap_busstop_filtered` data by using `honeycomb_grid` function created in Section 4.1.

```{r}
honeycombgrid_wdap_filtered <- honeycomb_grid(wdap_busstop_filtered)
```

## Weekends/Holiday Morning Peak 11am to 2pm

The following code chunk utilizes `busstop_filtered` function and name the filtered dataset as `wehmp_busstop_filtered`. After filtering, `check_excluded_bus_stop` function is used to determine if any of the excluded bus stop numbers are still present in `wehmp_busstop_filtered`.

```{r}
wehmp_busstop_filtered <- busstop_filtered(wehmp_busstop)
check_excluded_bus_stops(wehmp_busstop_filtered, bus_stop_numbers_to_exclude)
```

Next, we generate a new `honeycombgrid_wehmp_filtered` data based on `wehmp_busstop_filtered` data by using `honeycomb_grid` function created in Section 4.1.

```{r}
honeycombgrid_wehmp_filtered <- honeycomb_grid(wehmp_busstop_filtered)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

The following code chunk utilizes `busstop_filtered` function and name the filtered dataset as `wehep_busstop_filtered`. After filtering, `check_excluded_bus_stop` function is used to determine if any of the excluded bus stop numbers are still present in `wehep_busstop_filtered`.

```{r}
wehep_busstop_filtered <- busstop_filtered(wehep_busstop)
check_excluded_bus_stops(wehep_busstop_filtered, bus_stop_numbers_to_exclude)
```

Next, we generate a new `honeycombgrid_wehep_filtered` data based on `wehep_busstop_filtered` data by using `honeycomb_grid` function created in Section 4.1.

```{r}
honeycombgrid_wehep_filtered <- honeycomb_grid(wehep_busstop_filtered)
```
:::

## 5. Exploratory Spatial Data Analysis

Exploratory Spatial Data Analysis (ESDA) extends the principles of Exploratory Data Analysis to encompass descriptive techniques focused on revealing the spatial patterns within data and identifying any unusual observations.

In this section, we will explore both **global** and **local spatial autocorrelation**. The global perspective helps us identify broad patterns, while the local analysis enables us to identify specific areas with high or low values in the dataset.

### 5.1. **Global Spatial Autocorrelation**

In accordance with *Tobler*'s First Law of Geography, it is acknowledged that "all things are interconnected, but objects in close proximity exhibit stronger connections than those farther apart.".

In this subsection, we delve into the calculation of global spatial autocorrelation statistics and the assessment of spatial randomness on a global scale. The objective of these analyses is to gain insights into the distribution of passenger volume by Origin Destination Bus Stops across Singapore, assessing whether they are uniformly dispersed or not.

#### 5.1.1. **Spatial Weights Matrix**

To perform global spatial autocorrelation analysis, our initial step involves the creation of spatial weights for Singapore to establish the spatial neighborhood, a fundamental element for subsequent spatial analysis. There are two prevalent approaches for generating spatial weights: **contiguity-based** and **distance-based** methods.

In **contiguity-based** methods, neighboring areas are determined based on **shared boundaries**, with variations in how these boundaries are defined in different methods. **Rook** contiguity, for instance, considers neighbors that share a common edge. Conversely, **Queen** contiguity deems neighbors as those who share either a common vertex or edge. Consequently, Queen contiguity is more inclusive compared to Rook contiguity.

The distinctions between these two approaches are elucidated in the following illustration.

![](images/Comparing-rook-and-queen-contiguity-Each-square-is-representative-of-geographic-shapes.ppm){width="545"}

In **distance-based** contiguity, there are two distinct weighting schemes: **fixed** weighting, **adaptive** weighting and **inverse** weighting. The **fixed** distance approach deems two regions as neighbors if they fall within a **predetermined distance** of each other. In the **adaptive** distance scheme, every region is assigned an **equal number of neighbors**, with the specific number of neighbors predetermined. For instance, if we set k = 8 neighbors, it will categorize the nearest eight regions as neighbors for each. The **inverse** distance weight matrix is used to measure spatial correlation by assigning weights to neighboring observations based on the **inverse of their distances**. In this matrix, **closer** observations receive **higher** weights, indicating a **stronger** influence on each other's values, while more **distant** observations have **lower** weights, reflecting a **weaker** spatial relationship.

The choice of which spatial weights method to employ hinges on the geographical characteristics of the area under consideration. In cases where the geographical area comprises numerous isolated areas, opting for a contiguity-based matrix might lead to many regions lacking neighbors. Similarly, if the features (polygons) exhibit a wide range of sizes, encompassing both very large and relatively smaller ones, the use of a contiguity-based matrix may result in larger features having a disproportionate number of neighbors. This could potentially introduce a smoothing effect due to the higher number of neighbors for larger features, potentially skewing the analytical outcomes.

#### 5.1.2. **Contiguity-based Spatial Weights**

This section focuses on the creation of contiguity spatial weights using the **sfdep** package. Deriving contiguity spatial weights involves two key steps:

1.  Identifying the contiguity neighbor list using the [`st_contiguity()`](https://sfdep.josiahparry.com/reference/st_contiguity.html) function from the **sfdep** package.

2.  Generating the contiguity spatial weights using the [`st_weights()`](https://sfdep.josiahparry.com/reference/st_weights.html) function, also part of the **sfdep** package.

Throughout this section, we delve into the process of obtaining the contiguity neighbor list and the contiguity spatial weights separately. Subsequently, we will explore how to seamlessly combine both steps into a single procedure.

#### 5.1.2.1. **Contiguity-based (Queen) Spatial Weight Contiguity**

Next, we prepare a reusable function named `nb_queen` by utilizing [`st_contiguity()`](https://sfdep.josiahparry.com/reference/st_contiguity.html) to derive a contiguity neighbor list by using **Queen**'s method.

```{r}
nb_queen <- function(input_data){
  output <- input_data %>% 
    mutate(nb = st_contiguity(area_honeycomb_grid_current),
         .before = 1)
  
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

The following code chunk calculates **Queen** contiguity spatial weights for `honeycombgrid_wdmp_filtered` dataset and summarizes the neighboring regions for each hexagon.

```{r}
nb_queen_wdmp <- nb_queen(honeycombgrid_wdmp_filtered)
summary(nb_queen_wdmp$nb)
```

The summary report above shows that there are 1495 area units in Singapore. The most connected area units have 6 neighbors. There are 40 area units with only one neighbor.

The following code chunk uses the `kable` function to display the first 10 rows of the `nb_queen_wdmp` dataset in a tabular format.

```{r}
kable(head(nb_queen_wdmp, n=10))
```

## Weekday Afternoon Peak 5pm to 8pm

The following code chunk calculates **Queen** contiguity spatial weights for `honeycombgrid_wdap_filtered` dataset and summarizes the neighboring regions for each hexagon.

```{r}
nb_queen_wdap <- nb_queen(honeycombgrid_wdap_filtered)
summary(nb_queen_wdap$nb)
```

The summary report above shows that there are 1491 area units in Singapore. The most connected area units have 6 neighbors. There are 40 area units with only one neighbor.

The following code chunk uses the `kable` function to display the first 10 rows of the `nb_queen_wdap` dataset in a tabular format.

```{r}
kable(head(nb_queen_wdap, n=10))
```

## Weekends/Holiday Morning Peak 11am to 2pm

The following code chunk calculates **Queen** contiguity spatial weights for `honeycombgrid_wehmp_filtered` dataset and summarizes the neighboring regions for each hexagon.

```{r}
nb_queen_wehmp <- nb_queen(honeycombgrid_wehmp_filtered)
summary(nb_queen_wehmp$nb)
```

The summary report above shows that there are 1495 area units in Singapore. The most connected area units have 6 neighbors. There are 40 area units with only one neighbor.

The following code chunk uses the `kable` function to display the first 10 rows of the `nb_queen_wehmp` dataset in a tabular format.

```{r}
kable(head(nb_queen_wehmp, n=10))
```

## Weekends/Holiday Evening Peak 4pm to 7pm

The following code chunk calculates **Queen** contiguity spatial weights for `honeycombgrid_wehep_filtered` dataset and summarizes the neighboring regions for each hexagon.

```{r}
nb_queen_wehep <- nb_queen(honeycombgrid_wehep_filtered)
summary(nb_queen_wehep$nb)
```

The summary report above shows that there are 1488 area units in Singapore. The most connected area units have 6 neighbors. There are 41 area units with only one neighbor.

The following code chunk uses the `kable` function to display the first 10 rows of the `nb_queen_wehep` dataset in a tabular format.

```{r}
kable(head(nb_queen_wehep, n=10))
```
:::

#### 5.1.2.2. **Contiguity-based (Rook) Spatial Weight Contiguity**

Next, we prepare a reusable function named `nb_rook` by utilizing [`st_contiguity()`](https://sfdep.josiahparry.com/reference/st_contiguity.html) to derive a contiguity neighbor list by using **Rook**'s method.

```{r}
nb_rook <- function(input_data){
  output_nb_rook <- input_data %>% 
    mutate(nb = st_contiguity(area_honeycomb_grid_current,
                              queen = FALSE),
         .before = 1)
  
  return(output_nb_rook)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

The following code chunk calculates **Rook** contiguity spatial weights for `honeycombgrid_wdmp_filtered` dataset and summarizes the neighboring regions for each hexagon.

```{r}
nb_rook_wdmp <- nb_rook(honeycombgrid_wdmp_filtered)
summary(nb_rook_wdmp$nb)
```

The summary report above shows that there are 1490 area units in Singapore. The most connected area units have 6 neighbors. There are 40 area units with only one neighbor.

The following code chunk uses the `kable` function to display the first 10 rows of the `nb_rook_wdmp` dataset in a tabular format.

```{r}
kable(head(nb_rook_wdmp, n=10))
```

## Weekday Afternoon Peak 5pm to 8pm

The following code chunk calculates **Rook** contiguity spatial weights for `honeycombgrid_wdap_filtered` dataset and summarizes the neighboring regions for each hexagon.

```{r}
nb_rook_wdap <- nb_rook(honeycombgrid_wdap_filtered)
summary(nb_rook_wdap$nb)
```

The summary report above shows that there are 1491 area units in Singapore. The most connected area units have 6 neighbors. There are 40 area units with only one neighbor.

The following code chunk uses the `kable` function to display the first 10 rows of the `nb_rook_wdap` dataset in a tabular format.

```{r}
kable(head(nb_rook_wdap, n=10))
```

## Weekends/Holiday Morning Peak 11am to 2pm

The following code chunk calculates **Rook** contiguity spatial weights for `honeycombgrid_wehmp_filtered` dataset and summarizes the neighboring regions for each hexagon.

```{r}
nb_rook_wehmp <- nb_rook(honeycombgrid_wehmp_filtered)
summary(nb_rook_wehmp$nb)
```

The summary report above shows that there are 1495 area units in Singapore. The most connected area units have 6 neighbors. There are 40 area units with only one neighbor.

The following code chunk uses the `kable` function to display the first 10 rows of the `nb_rook_wehmp` dataset in a tabular format.

```{r}
kable(head(nb_rook_wehmp, n=10))
```

## Weekends/Holiday Evening Peak 4pm to 7pm

The following code chunk calculates **Rook** contiguity spatial weights for `honeycombgrid_wehep_filtered` dataset and summarizes the neighboring regions for each hexagon.

```{r}
nb_rook_wehep <- nb_rook(honeycombgrid_wehep_filtered)
summary(nb_rook_wehep$nb)
```

The summary report above shows that there are 1488 area units in Singapore. The most connected area units have 6 neighbors. There are 41 area units with only one neighbor.

The following code chunk uses the `kable` function to display the first 10 rows of the `nb_rook_wehep` dataset in a tabular format.

```{r}
kable(head(nb_rook_wehep,n=10))
```
:::

#### 5.1.3. **Distance-based Contiguity Weight Matrix**

#### 5.1.3.1. **Fixed Distance Weight Matrix**

Now, we will go ahead to derive summary statistics of the nearest neighbor distances vector (i.e. dists) by using the reusable `wmfd_dists` function in the following code chunk.

```{r}
wmfd_dists <- function(input_data){
  geo <- sf::st_geometry(input_data)
  nb <- st_knn(geo, longlat = TRUE)
  dists <- unlist(st_nb_dists(geo, nb))
  # summary(dists)
  return(summary(dists))
}

```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

The following code chunk applies `wmfd_dists` function on `honeycombgrid_wdmp_filtered` dataset.

```{r}
wmfd_dists(honeycombgrid_wdmp_filtered)
```

## Weekday Afternoon Peak 5pm to 9pm

The following code chunk applies `wmfd_dists` function on `honeycombgrid_wdap_filtered` dataset.

```{r}
wmfd_dists(honeycombgrid_wdap_filtered)
```

## Weekends/Holiday Morning Peak 11am to 2pm

The following code chunk applies `wmfd_dists` function on `honeycombgrid_wehmp_filtered` dataset.

```{r}
wmfd_dists(honeycombgrid_wehmp_filtered)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

The following code chunk applies `wmfd_dists` function on `honeycombgrid_wehep_filtered` dataset.

```{r}
wmfd_dists(honeycombgrid_wehep_filtered)
```
:::

From the results above, the majority of hexagons seem to be uniformly spaced at 500 units apart since the minimum, first quartile, median, and third quartile are all the same (500 units). This indicates a high level of uniformity in the spacing of most hexagons. The maximum value (2291.3 units) is an outlier compared to the other values. It suggests that there is at least one pair of hexagons that are significantly further apart than the rest.

The summary statistics report above shows that the maximum nearest neighbor distance is 2291.3 units. By using a threshold value of 2292 units will ensure that each area will have at least one neighbor. Now we will compute the **fixed** distance weights by using the reusable `wm_fd` function constructed in the following code chunk.

```{r}
wm_fd <- function(input_data) {
  output <- input_data %>%
    mutate(nb = st_dist_band(area_honeycomb_grid_current,
                        upper = 2292),
      wt = st_weights(nb),
      .before = 1)
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

The following code chunk applies `wm_fd` function on `honeycombgrid_wdmp_filtered` dataset.

```{r}
wm_fd(honeycombgrid_wdmp_filtered)
```

## Weekday Afternoon Peak 5pm to 8pm

The following code chunk applies `wm_fd` function on `honeycombgrid_wdap_filtered` dataset.

```{r}
wm_fd(honeycombgrid_wdap_filtered)
```

## Weekends/Holiday Morning Peak 11am to 2pm

The following code chunk applies `wm_fd` function on `honeycombgrid_wehmp_filtered` dataset.

```{r}
wm_fd(honeycombgrid_wehmp_filtered)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

The following code chunk applies `wm_fd` function on `honeycombgrid_wehep_filtered` dataset.

```{r}
wm_fd(honeycombgrid_wehep_filtered)
```
:::

#### 5.1.3.2. **Adaptive Distance-based Weight Matrix**

To address the challenge of an uneven distribution of neighbors in a **fixed** distance weight matrix, we can directly control the number of neighbors by employing the **k-nearest neighbors** approach, as demonstrated in the reusable function `wm_ad` in the following code chunk.

::: callout-important
In the case of a uniformly shaped hexagonal grid, the maximum allowable k value is 6, indicating that each region should ideally have precisely 6 neighboring regions. However, upon examining the visualizations in Section 4.1, it becomes evident that many hexagonal grids exhibit fewer than 6 neighbors. Given this observation, we have decided to adopt a value of **k = 4** for our subsequent analysis, taking into account the actual connectivity patterns.
:::

```{r}
wm_ad <- function(input_data) {
  output <- input_data %>%
    mutate(nb = st_knn(area_honeycomb_grid_current,
                  k = 4),
      wt = st_weights(nb),
      .before = 1)
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

The following code chunk applies `wm_ad` function on `honeycombgrid_wdmp_filtered` dataset.

```{r}
wm_ad(honeycombgrid_wdmp_filtered)
```

## Weekday Afternoon Peak 5pm to 8pm

The following code chunk applies `wm_ad` function on `honeycombgrid_wdap_filtered` dataset.

```{r}
wm_ad(honeycombgrid_wdap_filtered)
```

## Weekends/Holiday Morning Peak 11am to 2pm

The following code chunk applies `wm_ad` function on `honeycombgrid_wehmp_filtered` dataset.

```{r}
wm_ad(honeycombgrid_wehmp_filtered)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

The following code chunk applies `wm_ad` function on `honeycombgrid_wehep_filtered` dataset.

```{r}
wm_ad(honeycombgrid_wehep_filtered)
```
:::

#### 5.1.3.3. Inverse distance weights (IDW)

The following code chunk to construct reusable function `wm_idw` utilizes:

-   [`st_contiguity()`](https://sfdep.josiahparry.com/reference/st_contiguity.html) to identify the neighbors by using contiguity criteria. The output is a list of neighbors (i.e. nb).

-   [`st_inverse_distance()`](https://sfdep.josiahparry.com/reference/st_inverse_distance.html) to calculate **inverse** distance weights of neighbors on the nb list.

```{r}
wm_idw <- function(input_data) {
  output <- input_data %>%
    mutate(
      nb = st_contiguity(area_honeycomb_grid_current),
      wts = st_inverse_distance(nb,
        area_honeycomb_grid_current,
        scale = 1,
        alpha = 1),
      .before = 1)
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

The following code chunk applies `wm_idw` function on `honeycombgrid_wdmp_filtered` dataset.

```{r}
wm_idw(honeycombgrid_wdmp_filtered)
```

## Weekday Afternoon Peak 5pm to 8pm

The following code chunk applies `wm_idw` function on `honeycombgrid_wdap_filtered` dataset.

```{r}
wm_idw(honeycombgrid_wdap_filtered)
```

## Weekends/Holiday Morning Peak 11am to 2pm

The following code chunk applies `wm_idw` function on `honeycombgrid_wdap_filtered` dataset.

```{r}
wm_idw(honeycombgrid_wehmp_filtered)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

The following code chunk applies `wm_idw` function on `honeycombgrid_wehep_filtered` dataset.

```{r}
wm_idw(honeycombgrid_wehep_filtered)
```
:::

#### 5.1.4. **Which spatial weight matrix to use?**

The choice of a spatial weight matrix to use depends on the geographical area under investigation and the research objectives. In our specific context, when considering options between **contiguity-based** and **distance-based** spatial weight matrices, we are inclined towards the use of **distance-based** matrices. Within the category of distance-based matrices, we will specifically opt for the **adaptive** **distance-based** spatial weight matrix for our forthcoming analyses.

Here are the summarized reasons for our choice:

1.  Since the uniform grid has been disrupted by removing hexagons, an adaptive approach (like k-nearest neighbors) will be more appropriate. This method ensures that each hexagon has a set number of neighbors, compensating for any irregularities in the grid caused by the removal of areas with number of trip = 0.

2.  The adaptive method can effectively deal with the **non-contiguity issue**, ensuring that each hexagon still considers an appropriate neighborhood set, even if some adjacent hexagons are missing.

3.  With missing hexagons, **contiguity-based** methods (like **Rook** or **Queen**) might not accurately reflect the spatial relationships because they rely on the assumption of a c**ontinuous, uninterrupted grid**.

Taking these factors into account, our selection will be the **adaptive distance-based** spatial weight matrix.

#### 5.1.5. **Row-Standardized Weights Matrix**

With the weight matrix chosen, the next step is to assign weights to each neighboring polygon. In this process, every neighboring polygon will be given equal weight (**style="W"**) by allocating the fraction **1/(number of neighbors)** to each adjacent area. This corresponds to what is referred to as a **row-standardized** matrix, where each row in the matrix totals to 1.

The reusable `rswm_ad` function below takes input data and enhances it by incorporating spatial relationships through nearest neighbor calculations and weighting.

```{r}
rswm_ad <- function(input_data) {
  output <- input_data %>%
    mutate(nb = st_knn(area_honeycomb_grid_current,
                  k = 4),
      wt = st_weights(nb,
                      style = "W"),
      .before = 1)
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

The following code chunk applies `rswm_ad_wdmp` function on `honeycombgrid_wdmp_filtered` dataset.

```{r}
rswm_ad_wdmp <- rswm_ad(honeycombgrid_wdmp_filtered)
```

## Weekday Afternoon Peak 5pm to 8pm

The following code chunk applies `rswm_ad_wdmp` function on `honeycombgrid_wdap_filtered` dataset.

```{r}
rswm_ad_wdap <- rswm_ad(honeycombgrid_wdap_filtered)
```

## Weekends/Holiday Morning Peak 11am to 2pm

The following code chunk applies `rswm_ad_wdmp` function on `honeycombgrid_wehmp_filtered` dataset.

```{r}
rswm_ad_wehmp <- rswm_ad(honeycombgrid_wehmp_filtered)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

The following code chunk applies `rswm_ad_wdmp` function on `honeycombgrid_wehep_filtered` dataset.

```{r}
rswm_ad_wehep <- rswm_ad(honeycombgrid_wehep_filtered)
```
:::

#### 5.1.6. **Computing Global Spatial Autocorrelation Statistics**

In this subsection, we will employ two methods, namely **Moran's I** and **Geary's C**, to assess the following hypotheses:

-   **Null Hypothesis (H0)**: The observed spatial distribution of values is as likely as any other spatial arrangement, implying that the data is randomly distributed, devoid of discernible spatial patterns.

-   **Alternative Hypothesis (H1)**: The data exhibits a higher degree of spatial clustering than what would be expected by random chance alone.

#### 5.1.6.1. **Computing Global Moran' I**

In the code chunk below, the reusable `global_moran()` function is used to compute the **Moran's I** value. Different from **spdep** package, the output is a tibble data frame.

```{r}
moranI <- function(input_data) {
  output <- global_moran(input_data$TOT_TRIPS,
                       input_data$nb,
                       input_data$wt)
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

The following code chunk calculates **Moran's I** spatial autocorrelation statistic for the `rswm_ad_wdmp` dataset and displays its summary information using `glimpse`.

```{r}
moranI_wdmp <- moranI(rswm_ad_wdmp)
glimpse(moranI_wdmp)
```

The result with a **Moran's I** value of **0.218** for the dataset of **46** observations, indicates a **positive** spatial autocorrelation. This suggests that neighboring regions tend to have similar values, revealing **moderate** spatial patterns within the data.

## Weekday Afternoon Peak 5pm to 8pm

The following code chunk calculates **Moran's I** spatial autocorrelation statistic for the `rswm_ad_wdap` dataset and displays its summary information using `glimpse`.

```{r}
moranI_wdap <- moranI(rswm_ad_wdap)
glimpse(moranI_wdap)
```

The result, with a **Moran's I** value of **0.0764** for the dataset of **88.4** observations, indicates a **relatively low** level of spatial autocorrelation. This suggests that nearby regions in the dataset have some degree of similarity in their values, but the correlation is **not very strong**.

## Weekends/Holiday Morning Peak 11am to 2pm

The following code chunk calculates **Moran's I** spatial autocorrelation statistic for the `rswm_ad_wehmp` dataset and displays its summary information using `glimpse`.

```{r}
moranI_wehmp <- moranI(rswm_ad_wehmp)
glimpse(moranI_wehmp)
```

The result, with a **Moran's I** value of **0.173** and 51.4 observations, suggests **moderate** **positive** spatial autocorrelation. This indicates that neighboring regions in the dataset tend to have moderately similar values, indicating the presence of spatial patterns within the data.

## Weekends/Holiday Evening Peak 4pm to 7pm

The following code chunk calculates **Moran's I** spatial autocorrelation statistic for the `rswm_ad_wehep dataset` and displays its summary information using `glimpse`.

```{r}
moranI_wehep <- moranI(rswm_ad_wehep)
glimpse(moranI_wehep)
```

The result, with a **Moran's I** value of **0.122** and **73.1** observations, indicates a **moderate** **positive** spatial autocorrelation. This suggests that nearby regions in the dataset tend to have moderately similar values, indicating the presence of spatial patterns within the data.
:::

#### **5.1.6.1.1. Performing Global Moran's I test**

In general, **Moran's I** test will be performed instead of just computing the **Moran's I** statistics. With **sfdep** package, **Moran's I** test can be performed by using [`global_moran_test()`](https://sfdep.josiahparry.com/reference/global_moran_test.html) as shown in the reusable function `global_moranI` in the code chunk below.

```{r}
global_moranI <- function(input_data) {
  output <- global_moran_test(input_data$TOT_TRIPS,
                              input_data$nb,
                              input_data$wt)
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

The following code chunk computes the **Global Moran's I** statistic for the `rswm_ad_wdmp` dataset.

```{r}
global_moranI(rswm_ad_wdmp)
```

The **Global Moran's I** test result strongly supports the presence of significant **positive** spatial autocorrelation in the dataset, with a **Moran's I** statistic of approximately **0.218**. This indicates that neighboring regions exhibit similar values, forming distinct spatial clusters. The very low p-value (\< **2.2e-16**) highlights the statistical significance of this spatial pattern, while the standard deviate of around **12.824** underscores its deviation from spatial randomness.

## Weekday Afternoon Peak 5pm to 8pm

The following code chunk computes the **Global Moran's I** statistic for the `rswm_ad_wdap` dataset.

```{r}
global_moranI(rswm_ad_wdap)
```

The **Global Moran's I** test result indicates a statistically significant **positive** spatial autocorrelation within the dataset. The **Moran's I** statistic of approximately **0.0764** suggests that neighboring regions tend to exhibit similar values, signifying a spatial pattern where similar attributes or characteristics cluster together geographically. The extremely low p-value (**2.147e-06**) underscores the statistical significance of this spatial pattern, indicating that it is highly unlikely to be a result of random chance. The standard deviate of around **4.5967** further supports the significant deviation from spatial randomness, confirming the presence of positive spatial autocorrelation in the data.

## Weekends/Holiday Morning Peak 11am to 2pm

The following code chunk computes the **Global Moran's I** statistic for the `rswm_ad_wehmp` dataset.

```{r}
global_moranI(rswm_ad_wehmp)
```

The **Global Moran's I** test result reveals a highly significant **positive** spatial autocorrelation within the dataset. The computed **Moran's I** statistic, approximately **0.173**, signifies that nearby regions tend to exhibit similar values, indicating a distinct spatial clustering pattern. The extremely low p-value (**\< 2.2e-16**) underscores the statistical significance of this spatial pattern, suggesting that it is exceptionally unlikely to occur by random chance. The standard deviate of approximately **10.232** reinforces the substantial deviation from spatial randomness, affirming the presence of significant positive spatial autocorrelation in the data.

## Weekends/Holiday Evening Peak 4pm to 7pm

The following code chunk computes the **Global Moran's I** statistic for the `rswm_ad_wehep` dataset.

```{r}
global_moranI(rswm_ad_wehep)
```

The **Global Moran's I** test result indicates a statistically significant **positive** spatial autocorrelation within the dataset. The computed **Moran's I** statistic, approximately **0.1224**, suggests that neighboring regions tend to exhibit similar values, implying a discernible spatial clustering pattern. The low p-value (**1.566e-13**) underscores the statistical significance of this spatial pattern, indicating that it is highly unlikely to occur by random chance. The standard deviate of around **7.2886** further emphasizes the substantial deviation from spatial randomness, confirming the presence of significant positive spatial autocorrelation in the data.
:::

#### 5.1.6.1.2. **Performing Global Moran'I permutation test**

In practice, **Monte Carlo** simulation should be used to perform the statistical test. For **sfdep**, it is supported by `globel_moran_perm()`.

::: callout-note
It is always a good practice to use `set.seed()` before performing simulation. This is to ensure that the computation is reproducible.
:::

```{r}
set.seed(1234)
```

Next, the following reusable function called `global_moranI_perm` utilizes `global_moran_perm()` to perform **Monte Carlo** simulation.

```{r}
global_moranI_perm <- function(input_data) {
  output <- global_moran_perm(input_data$TOT_TRIPS,
                              input_data$nb,
                              input_data$wt,
                              nsim = 99)
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

The following code chunk calculates the **Global Moran's I** statistic with **permutations** for the `rswm_ad_wdmp` dataset.

```{r}
global_moranI_perm(rswm_ad_wdmp)
```

The result of the **Global Moran's I** permutation test indicates that there is a statistically significant spatial autocorrelation in the dataset. The observed **Moran's I** statistic of approximately **0.21769** suggests that nearby regions tend to have similar values, revealing a spatial clustering pattern. The very low p-value (**\< 2.2e-16**) reinforces the significance of this spatial pattern, indicating that it is highly unlikely to be a result of random chance. The analysis confirms that the observed spatial clustering is a meaningful and non-random pattern within the data.

## Weekday Afternoon Peak 5pm to 8pm

The following code chunk calculates the **Global Moran's I** statistic with **permutations** for the `rswm_ad_wdap` dataset.

```{r}
global_moranI_perm(rswm_ad_wdap)
```

The result of the **Global Moran's I** permutation test suggests a significant positive spatial autocorrelation in the dataset. The **Moran's I** statistic of approximately **0.076421** indicates that neighboring regions tend to exhibit similar values, signifying a spatial clustering pattern. The very low p-value (**\< 2.2e-16**) underscores the statistical significance of this spatial pattern, suggesting that it is highly unlikely to be a result of random chance.

## Weekends/Holiday Morning Peak 11am to 2pm

The following code chunk calculates the **Global Moran's** **I** statistic with **permutations** for the `rswm_ad_wehmp` dataset.

```{r}
global_moranI_perm(rswm_ad_wehmp)
```

The result of the **Global Moran's I** permutation test reveals a substantial positive spatial autocorrelation in the dataset. The computed **Moran's I** statistic, approximately **0.17296**, indicates that neighboring regions tend to exhibit similar values, indicating a discernible spatial clustering pattern. The extremely low p-value (**\< 2.2e-16**) underscores the statistical significance of this spatial pattern, suggesting that it is highly improbable to result from random chance.

## Weekends/Holiday Evening Peak 4pm to 7pm

The following code chunk calculates the **Global Moran's I** statistic with **permutations** for the `rswm_ad_wehep` dataset.

```{r}
global_moranI_perm(rswm_ad_wehep)
```

The result of the **Global Moran's I** permutation test indicates a significant positive spatial autocorrelation in the dataset. The **Moran's I** statistic, approximately **0.12238**, suggests that neighboring regions tend to have similar values, highlighting a spatial clustering pattern. The extremely low p-value (**\< 2.2e-16**) emphasizes the statistical significance of this spatial pattern, indicating that it is highly unlikely to be a result of random chance.
:::

The report above show that the p-value is smaller than alpha value of 0.05. Hence, we reject the null hypothesis that the spatial patterns spatial independent. Because the **Moran's I** statistics is greater than 0. We can infer the spatial distribution shows sign of clustering.

#### 5.1.6.2. **Computing Local Moran's I**

In this section, we will compute **Local Moran's I** of total number of passenger trips by using [`local_moran()`](https://sfdep.josiahparry.com/reference/local_moran.html) of **sfdep** package. We construct a reusable function called `lisa` as shown in the following code chunk to compute **Local Indicators of Spatial Association (LISA)** statistics.

```{r}
lisa <- function(input_data) {
  output <- input_data %>%
    mutate(local_moran = local_moran(TOT_TRIPS, nb, wt, nsim = 99),
           .before = 1) %>%
    unnest(local_moran)
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

The following code chunk computes **LISA** statistics for the `rswm_ad_wdmp` dataset and displays the results.

```{r}
lisa_wdmp <- lisa(rswm_ad_wdmp)
lisa_wdmp
```

## Weekday Afternoon Peak 5pm to 8pm

The following code chunk computes **LISA** statistics for the `rswm_ad_wdap` dataset and displays the results.

```{r}
lisa_wdap <- lisa(rswm_ad_wdap)
lisa_wdap
```

## Weekends/Holiday Morning Peak 11am to 2pm

The following code chunk computes **LISA** statistics for the `rswm_ad_wehmp` dataset and displays the results.

```{r}
lisa_wehmp <- lisa(rswm_ad_wehmp)
lisa_wehmp 
```

## Weekends/Holiday Evening Peak 4pm to 7pm

The following code chunk computes **LISA** statistics for the `rswm_ad_wehep` dataset and displays the results.

```{r}
lisa_wehep <- lisa(rswm_ad_wehep)
lisa_wehep 
```
:::

The output of `local_moran()` is a sf data frame containing the columns *ii*, *eii*, *var_ii*, *z_ii*, *p_ii*, *p_ii_sim*, and *p_folded_sim*.

-   *ii*: local moran statistic

-   *eii*: expectation of local moran statistic; for `localmoran_perm`, the permutation sample means

-   *var_ii*: variance of local moran statistic; for `localmoran_perm`, the permutation sample standard deviations

-   *z_ii*: standard deviate of local moran statistic; for `localmoran_perm` based on permutation sample means and standard deviations

-   *p_ii*: p-value of local moran statistic using `pnorm()`; for `localmoran_perm` using standard deviates based on permutation sample means and standard deviations

-   *p_ii_sim*: For `localmoran_perm()`, `rank()` and `punif()` of observed statistic rank for \[0, 1\] p-values using `alternative=` -p_folded_sim: the simulation folded \[0, 0.5\] range ranked p-value

-   *skewness*: For `localmoran_perm`, the output of e1071::`skewness()` for the permutation samples underlying the standard deviates

-   *kurtosis*: For `localmoran_perm`, the output of e1071::`kurtosis()` for the permutation samples underlying the standard deviates.

#### 5.1.6.2.1. Visualizing **local Moran's I**

In this code chunk below, **tmap** functions are used prepare a choropleth map by using value in the *ii* field.

```{r}
visual_local_moranI <- function(input_data) {
  output <- tmap_mode("view") +
    tm_shape(input_data) +
    tm_fill("ii",
            title = "Local Moran's I ii values") +
    tm_borders(alpha = 0.5) +
    tm_view(set.zoom.limits = c(11, 20)) +
    tm_layout(main.title = "local Moran's I of Total Trips",
              main.title.size = 0.8)
  return(output)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
visual_local_moranI(lisa_wdmp)
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
visual_local_moranI(lisa_wdap)
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
visual_local_moranI(lisa_wehmp)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
visual_local_moranI(lisa_wehep)
```
:::

-   **Positive "ii" Value**: Indicates spatial clustering. A positive value suggests that a location has neighboring values that are similar to itself. For example, a bus stop with a high number of passenger trips (higher than the mean) surrounded by bus stops with similarly high passenger counts would result in a positive **Local Moran's I** value for that location, indicating a *high-high* cluster. Similarly, a *low-low* cluster occurs when a location with a low value is surrounded by neighbors with low values.

-   **Zero "ii" Value**: A *zero* or *near-zero* value suggests no significant spatial autocorrelation at that location. This means the number of passenger trips at a bus stop is neither significantly higher nor lower than its neighbors, or that the distribution is random.

-   **Negative "ii" Value**: Indicates spatial **outliers**. A negative value means that a location is significantly different from its neighbors. For instance, a bus stop with a high number of passenger trips surrounded by bus stops with low passenger counts would have a negative **Local Moran's I** value, labeled as a *high-low* **outlier**. Conversely, a *low-high* **outlier** occurs when a location with a low value is surrounded by neighbors with high values.

#### 5.1.6.2.2. Visualizing **p-value of local Moran's I**

In the code chunk below, **tmap** functions are used prepare a choropleth map by using value in the *p_ii_sim* field.

```{r}
visual_local_moranI_pvalue <- function(input_data) {
  tmap_mode("view") +
    tm_shape(input_data) +
    tm_fill(
      "p_ii_sim",
      breaks = c(-Inf, 0.01, 0.05, 0.1, Inf),
      title = "Local Moran's I p-values"
    ) +
    tm_borders(alpha = 0.5) +
    tm_layout(main.title = "Local Moran's I p-values",
              main.title.size = 0.8)
}
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
visual_local_moranI_pvalue(lisa_wdmp)
```

## Weekday Afternoon Peak 5pm to 8pm

```{r}
visual_local_moranI_pvalue(lisa_wdap)
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
visual_local_moranI_pvalue(lisa_wehmp)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
visual_local_moranI_pvalue(lisa_wehep)
```
:::

-   **Dark Orange Hexagons (0.10 or more)**: These areas have p-values equal to or greater than 0.10, indicating that the spatial patterns observed here are not statistically significant. The high p-values suggest that the observed spatial clustering or outliers could very well be due to random chance rather than a meaningful spatial process.

-   **Orange Hexagons (0.05 to 0.10)**: The orange hexagons have p-values between 0.05 and 0.10, which are considered to be marginally significant. These could be areas where the spatial patterns are possibly meaningful, but the evidence is not strong enough to confidently rule out random variation.

-   **Light Orange Hexagons (0.01 to 0.05)**: Light orange hexagons indicate areas where the p-values are low, between 0.01 and 0.05. These are generally considered statistically significant, suggesting that the observed spatial clusters or outliers are likely to reflect real spatial patterns and not just random distribution.

-   **Yellow Hexagons (Less than 0.01)**: These hexagons have the lowest p-values, less than 0.01, indicating a very high level of statistical significance. The spatial patterns in these areas are highly unlikely to be due to random chance, and there is a strong indication of true spatial clustering or outlier phenomena.

#### 5.1.6.2.3. Visualizing **local Moran's I and p-value**

For effective comparison, we will plot both maps next to each other as shown below.

```{r}

map1 <- function(input_data) {
  tmap_mode("view") +
  tm_shape(input_data) +
    tm_fill("ii",
            title = "Local Moran's I ii values") +
    tm_borders(alpha = 0.5) +
    tm_view(set.zoom.limits = c(10, 15)) +
    tm_layout(main.title = "local Moran's I of Total Trips",
              main.title.size = 0.8)
}


map2 <- function(input_data) {
  tmap_mode("view") +
    tm_shape(input_data) +
    tm_fill(
      "p_ii_sim",
      breaks = c(-Inf, 0.01, 0.05, 0.1, Inf),
      title = "Local Moran's I p-values"
    ) +
    tm_borders(alpha = 0.5) +
    tm_layout(main.title = "Local Moran's I p-values",
              main.title.size = 0.8)
}

```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
tmap_arrange(map1(lisa_wdmp), map2(lisa_wdmp), ncol = 2)
```

The **Local Moran's I** *ii* values map highlights an **outlier** on the western side of Singapore. However, a p-value of **0.58** suggests that the outlier is not statistically significant, implying that the observation could indeed be due to random variation. Additionally, northern, southwestern, and northeastern regions of Singapore which have positive *ii* values have p-values of less than **0.05** indicating that the spatial autocorrelation is statistically significant.

## Weekday Afternoon Peak 5pm to 8pm

```{r}
tmap_arrange(map1(lisa_wdap), map2(lisa_wdap), ncol = 2)
```

The **Local Moran's I** *ii* values map highlights an **outlier** on the western side of Singapore. However, a p-value of **0.2** suggests that the outlier is not statistically significant, implying that the observation could indeed be due to random variation. Additionally, northern, region of Singapore which has distinct positive *ii* value has p-value of less than **0.05** indicating that the spatial autocorrelation is statistically significant.

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
tmap_arrange(map1(lisa_wehmp), map2(lisa_wehmp), ncol = 2)
```

The **Local Moran's I** *ii* values map highlights an **outlier** on the western side of Singapore. However, a p-value of **0.28** suggests that the outlier is not statistically significant, implying that the observation could indeed be due to random variation. Additionally, the eastern and northeastern regions of Singapore which have positive *ii* values have p-values of less than **0.05** indicating that the spatial autocorrelation is statistically significant.

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
tmap_arrange(map1(lisa_wehep), map2(lisa_wehep), ncol = 2)
```

The **Local Moran's I** *ii* values map highlights an **outlier** on the western side of Singapore. However, a p-value of **0.22** suggests that the outlier is not statistically significant, implying that the observation could indeed be due to random variation. Additionally, the northern and eastern regions of Singapore which have positive *ii* values have p-values of less than **0.05** indicating that the spatial autocorrelation is statistically significant.
:::

#### 5.1.6.2.4. Visualizing **LISA map**

**LISA** map is a categorical map showing outliers and clusters. There are two types of outliers namely: *high-low* and *low-high* outliers. Likewise, there are two type of clusters namely: *high-high* and *low-low* clusters. In fact, **LISA** map is an interpreted map by combining **Local Moran's I** of geographical areas and their respective p-values.

In lisa sf data frame, we can find three fields contain the **LISA** categories. They are *mean*, *median* and *pysal*. In general, classification in *mean* will be used as shown in the code chunk below.

```{r}
visualize_lisa_map <- function(input_data) {
  lisa_sig <- input_data  %>%
    filter(p_ii_sim < 0.05)
  tmap_mode("view")
  tm_shape(input_data) +
    tm_polygons() +
    tm_borders(alpha = 0.5) +
    tm_shape(lisa_sig) +
    tm_fill("mean") +
    tm_borders(alpha = 0.4)
}  
```

::: panel-tabset
## Weekday Morning Peak 6am to 9am

```{r}
visualize_lisa_map(lisa_wdmp)
```

Low-Low clusters: Western region (Tuas, Pioneer)

## Weekday Afternoon Peak 5pm to 8pm

```{r}
visualize_lisa_map(lisa_wdap)
```

## Weekends/Holiday Morning Peak 11am to 2pm

```{r}
visualize_lisa_map(lisa_wehmp)
```

## Weekends/Holiday Evening Peak 4pm to 7pm

```{r}
visualize_lisa_map(lisa_wehep)
```
:::

-   **Low-Low**: Areas where the value (e.g., passenger trips) is low and is surrounded by areas with similarly low values, indicating clusters of low values.

-   **High-Low**: Areas where the value is high but is surrounded by areas with low values, identifying spatial outliers.

-   **Low-High**: Areas where the value is low but is surrounded by areas with high values, also identifying spatial outliers.

-   **High-High**: Areas where the value is high and is surrounded by areas with similarly high values, indicating clusters of high values.

#### 5.1.6.2.5. Interpretation of Results

Through this analysis, it has been observed that:

During weekday mornings from 6am to 9am, a hexagonal grid displaying a *High-Low* pattern was noted, possibly linked to the substantial number of workers entering Singapore from Malaysia via the Tuas checkpoint. *Low-Low* clusters are present in the west and northwest, which are predominantly industrial zones and typically the end points of these commuters' morning journeys, resulting in lower trip origins from these locales. A significant H*igh-High* cluster is evident near the Woodlands checkpoint, likely due to Malaysian workers commuting to Singapore. Various *Low-High* and *High-High* regions are also scattered throughout Singapore during this time.

In the weekday afternoon peak hours from 5pm to 8pm, contrary to expectations, *Low-Low* clusters persist in the western and northwestern sectors, suggesting that the workers may be utilizing company-provided shuttles or private transportation for their homeward commutes, rather than public bus services.

On weekends/holidays during the late morning peak from 11am to 2pm, the western and northwestern regions maintain *Low-Low* cluster statuses, aligning with the non-working day pattern. A *High-Low* cluster is noticeable in an area known for worker lodging, which may correspond with workers departing their residences. Near the Woodlands checkpoint, a *High-High* cluster emerges, possibly attributable to Malaysians traveling home or Singaporeans visiting Malaysia.

In the weekends/holidays evening peak times from 4pm to 7pm, a *High-Low* hexagonal grid is again identified, this time potentially due to workers from Malaysia arriving via the Tuas checkpoint in preparation for work the following day. A *Low-High* cluster around Sentosa island indicates an outflow of visitors in the evening.

## 6. Conclusion

From the analysis and visualization, urban planners and policymakers can determine which areas have significantly high or low values and where there may be outliers. This can inform resource allocation, service delivery, and further investigation into why certain areas might be under- or over-performing. *High-High* areas may need more resources or infrastructure to support the high usage, while *Low-Low* areas may require development or marketing efforts to increase usage. *High-Low* and *Low-High* areas represent **outliers** that might be investigated for unique local characteristics or issues.

### 6.1. Recommendation and Future Work

For recommendations and further work, it would be prudent to investigate the feasibility of enhancing public transit options during peak weekday mornings and evenings, especially in High-Low and Low-High areas to better serve cross-border commuters. Additionally, the consistent Low-Low clusters in industrial areas during the afternoon peaks suggest a need to understand and potentially integrate private transportation systems with public options. On weekends/holidays, the transit patterns near lodging and checkpoints could be studied to optimize cross-border travel. Continual monitoring of these patterns can inform targeted improvements to the transit network, considering both temporal and spatial variations in ridership.
